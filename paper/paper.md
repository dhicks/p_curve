---
title: "Young's p-value plot does not provide evidence against air pollution hazards"
#author: "Daniel J. Hicks"
abstract: >
  S. Stanley Young and collaborators have criticized epidemiological studies and meta-analyses of air pollution hazards using a graphical method they call a p-value plot, claiming to find zero effects, heterogeneity, and p-hacking.  However, the p-value plot method has not been validated in a peer-reviewed publication. The primary aim of this study was to investigate the statistical and evidentiary properties of this method. A simulation was developed to create studies and meta-analyses with known real effects $\delta$, integrating two quantifiable conceptions of evidence from the philosophy of science literature. The simulation and analysis is publicly available and automatically reproduced.  In this simulation, Young's p-value plot did not provide evidence for heterogeneity or p-hacking with respect to any condition. Under the right conditions, Young's p-value plot can provide evidence of zero effects; but these conditions are not satisfied in any actual use by Young and collaborators. Young's p-value plot does not provide evidence to support the skeptical claims about air pollution hazards made by Young and collaborators.  
bibliography: Young.bib
header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
tblPrefix:
  - "table"
  - "tables"
---

\renewcommand{\P}{\ensuremath{\mathbb{P}}}
\newcommand{\SchSp}{Schweder and Spjøtvoll}

# Introduction # {#sec:intro}

In numerous recent papers [@YoungCerealinducedGenderSelection2009; @YoungAssessingGeographicHeterogeneity2013; @YouPM2OzoneIndicators2018; @YoungReliabilityEnvironmentalEpidemiology2019; @YoungAmbientAirPollution2019; @YoungCombinedBackgroundInformation2019; @YoungEvaluationMetaanalysisAir2019; @YoungReExtendedMortality2020; @YoungPM2AllcauseMortality2020], statistician S. Stanley Young and collaborators have criticized epidemiological studies and meta-analyses of the harmful effects of air pollution.  Young and collaborators have used a graphical method they call a p-value plot, claiming that this method reveals zero effects, heterogeneity, and p-hacking in the primary air pollution literature.^[Here *heterogeneity* is used in the statistical sense, meaning that real effects vary across subsamples/subpopulations.  For example, the harmful effects of exposure to a given level of air pollution might be different in different places.  *P-hacking* refers to the bad statistical habit of trying different analyses — for example, including or excluding different covariates — until a "statistically significant" result is produced [@SimmonsFalsePositivePsychology2011].  This approach violates the standard assumptions made by statistical hypothesis tests, so that the p-value cannot be interpreted in terms of the type I error rate.]

Young and collaborators have drawn highly skeptical conclusions about the hazards of air pollution using their p-value plot method, claiming that "causality of PM10/PM2.5 on heart attacks is not supported" [@YoungReliabilityEnvironmentalEpidemiology2019] and "There is no convincing evidence of an effect of PM2.5 on all-cause mortality" [@YoungPM2AllcauseMortality2020]; Young has advocated that "regulation of PM2.5 should be abandoned altogether" [@YoungSuggestionsEPA2017].  While recent papers in this body of work have not yet been highly cited in the academic literature, one paper by Young and collaborators [@YoungAirQualityAcute2017] was cited in the scientific review of US EPA's Ozone Integrated Science Assessment [@USEPACleanAirScientificAdvisoryCommitteeReviewEPAIntegrated2020, p.D-50]; this review was conducted while Young was serving on US EPA's Science Advisory Board [@USEPAMembersScienceAdvisory2017].  In addition, at least some of Young's work on air pollution has been funded by the American Petroleum Institute [@YoungAirQualityEnvironmental2017; @YoungAirQualityAcute2017; @YouPM2OzoneIndicators2018; @YoungReliabilityEnvironmentalEpidemiology2019; @YoungEvaluationMetaanalysisAir2019].  For these reasons, it is highly plausible that Young's work could be cited in the future by opponents of air pollution regulation.  

However, Young and collaborators have provided only a minimal analysis of the statistical properties of the p-value plot method, in a set of publicly available but non-peer-reviewed notes [@YoungCombinedBackgroundInformation2019].  They have sometimes attempted to justify their method by citing plots of p-values developed by other authors [@SchwederPlotsPvaluesEvaluate1982; @SimonsohnPcurveKeyFiledrawer2014].  But these other plots are designed to answer different questions, are constructed in different ways, and have different statistical properties. 

In this paper, I formally evaluate the evidentiary value of the p-value plot method used by Young and collaborators.  For this purpose, numerical simulations offer a number of advantages over exact, purely formal methods.  First, for the target audience of this paper, well-written R code is likely to be more accessible than a purely formal derivation of the statistical properties of meta-analyses.  This makes it easier for readers to scrutinize the details of my analysis.  Second, the simulation is modular, making it more extensible or modifiable by readers than a formal analysis.  The simulated data generating process and its analysis can be modified by simply rewriting two short functions and re-running the simulation.  Any formal analysis of a different data-generating process might require a completely different proof strategy.  Third, it may be algebraically intractable to give formal characterizations of constructions such as plots except in the limit (as sample size and/or number of studies goes to infinity).  Simulations can produce good numerical estimates in a few minutes.  

Based on this simulation analysis, I find that the p-value plot method does not produce evidence to support the criticisms of air pollution epidemiology leveled by Young and collaborators.  

The paper proceeds as follows.  Section @sec:reconstruction briefly sets some context for the use of Young's p-value plot, then develops formal definitions of the plot as well as two other visual methods.  While Young and collaborators repeatedly refer to these other plots as though they were synonymous with Young's p-value plot, the formal definitions make it clear that the plots are quite different from each other.  So work that has been done to validate one plot does not validate either of the others.  Section @sec:simulation describes the simulation and reports and interprets its results.  


# Reconstructing Young's Methods # {#sec:reconstruction}

Young and collaborators have made three kinds of arguments criticizing air pollution epidemiology.  The most common argument first observes that analysts of complex data sets must make a number of decisions about which hypotheses to investigate and how to construct models; Young and collaborators then relate the large number of possible combinations of decisions to the traditional frequentist problem of correcting for multiple comparisons [@YoungCerealinducedGenderSelection2009; @YoungDemingDataObservational2011; @YoungReModelingAssociation2013; @PeaceReliabilityNutritionalMetaanalysis2017; @YoungAirQualityEnvironmental2017; @YoungAirQualityAcute2017; @YouPM2OzoneIndicators2018; @YoungAirPollutionMortality2018; @YoungReliabilityEnvironmentalEpidemiology2019; @YoungEvaluationMetaanalysisAir2019; @YoungReExtendedMortality2020].  For a critical assessment of this argument, see [redacted]<!--@HicksWhenVirtuesAre-->.  A second argument deploys a statistical method that Young and collaborators call "local control," which first clusters geographic units and then evaluates hazards separately within each cluster [@ObenchainAdvancingStatisticalThinking2013; @YoungBiasResponseHeterogeneity2015; @ObenchainLocalControlStrategy2017; @ObenchainLowlevelRadonExposure2019].  The local control method is not examined here.  

This paper focuses on the third argument made by Young and collaborators, which examines a collection of p-values, usually extracted from the primary studies in a meta-analysis or similar aggregate of studies [@YoungReliabilityEnvironmentalEpidemiology2019; @YoungAmbientAirPollution2019; @YoungCombinedBackgroundInformation2019; @YoungEvaluationMetaanalysisAir2019; @YoungReExtendedMortality2020; @YoungPM2AllcauseMortality2020] but sometimes produced by testing multiple distinct hypotheses on a single complex dataset [@YoungCerealinducedGenderSelection2009; @YoungAssessingGeographicHeterogeneity2013; @YouPM2OzoneIndicators2018]. The p-values are visualized using what Young and collaborators call a "p-value plot," and features of the plot are interpreted as indicating that there is no underlying effect [@YoungCerealinducedGenderSelection2009; @YouPM2OzoneIndicators2018; @YoungCombinedBackgroundInformation2019; @YoungReExtendedMortality2020; @YoungPM2AllcauseMortality2020], a heterogeneous mixture of zero and nonzero effects [@YoungReliabilityEnvironmentalEpidemiology2019; @YoungAmbientAirPollution2019; @YoungCombinedBackgroundInformation2019; @YoungEvaluationMetaanalysisAir2019], or that the authors of the primary studies have engaged in p-hacking [@YoungEvaluationMetaanalysisAir2019]. 

The first challenge in evaluating Young's p-value plot is the lack of exposition.  Young and collaborators appear to rely almost entirely on visual inspection of their p-value plots, and give vague characterizations of how they think features of the plots should be interpreted.  Their interpretations are rarely given a clear justification.  At least one key term in their exposition is used in a non-standard way, and the name "p-value plot" associates the method used by Young and collaborators with two other, entirely different graphical analyses of distributions of p-values.  

Therefore, in this section, I begin to develop a more precisely-defined set of methods, which can be implemented computationally and automatically reproduced.  These methods are intended to be more exact and reproducible — and hence intersubjectively stable — interpretations of the methods described by Young and collaborators.  Insofar as my versions are indeed more exact and intersubjectively stable versions of the latter methods, and insofar as my versions do not provide evidence to support the claims made by Young and collaborators, then I take it we have very good reason to conclude that the methods used by Young and collaborators do not provide evidence either.  That is, we would not expect less rigorous methods to provide better evidence than more rigorous methods.  

I proceed as follows.  I first briefly review two other graphical methods, cited by Young and collaborators in an attempt to justify their own method.  I then turn to Young's p-value plot, defining the plot itself and showing how it differs from the other two methods, and then discussing the various ways Young and collaborators have interpreted it.  

All three graphical methods take as input a set of $N$ p-values $\P = \{p_1, p_2, \ldots, p_N\}$, (nominally) produced by applying a given statistical hypothesis test to $N$ replications of a given study design, each replication drawing samples of size $n$ from a given population.  This corresponds to the simplest case of meta-analysis.  Thus the p-values in $\P$ are nominally samples from a single underlying distribution $p_i \sim P$.  Note that, if the real effect is zero $\delta = 0$, then $P$ is the uniform distribution on $[0,1]$.  

Young and collaborators have also applied Young's p-value plot to collections of p-values produced by, for example, conducting hundreds of possible analyses on a single dataset [@YoungCerealinducedGenderSelection2009].  This method resembles "multiverse analysis," which systematically examines how methodological decisions shape the outputs of statistical analysis [@SteegenIncreasingTransparencyMultiverse2016].  However, the point of multiverse analysis is to identify patterns in the associations between decisions and outputs.  Young and collaborators have drawn much simpler conclusions, such as that "all of the p-values are simply the result of chance" [@YoungCerealinducedGenderSelection2009].  Because simulating these applications would require modeling complex multivariate distributions, I do not consider these applications here.  

A brief note on the name "Young's p-value plot."  With one exception, every peer-reviewed publication using the method has been written by a collection of authors.  In each of these works the method has been referred to as a "p-value plot," without attributing it to any individual.  However, Young is the only author who appears across all of these papers; it does not appear to be widely used by other authors (e.g., Google Scholar searches for "p-value plot" generally turn up matches for other plots, especially Simonsohn, Nelson, and Simmons' p-curve); and in this section of the current paper the method needs to be distinguished from the other, nominally similar methods that Young and collaborators cite in an attempt to justify their plot.  So, in this section, I refer to their method as "Young's p-value plot."  Afterwards, except when there might be ambiguity, I simply use "the p-value plot" to refer to Young's specific plot.  

## Schweder and Spjøtvoll's p-value plot ##

Young and collaborators have frequently cited the "p-value plot" presented in @SchwederPlotsPvaluesEvaluate1982.  For this p-value plot, let $rank_{desc}(p_i)$ be the (1-indexed) *descending rank* of $p_i \in \P$, i.e., $rank_{desc}(p_i)$ is the number of p-values $p_j \in \P$ greater than or equal to $p_i$.  The largest p-value has descending rank 1, and the smallest p-value has descending rank $N$.  Then Schweder and Spjøtvoll's p-value plot plots the graph $(1-p_i, rank_{desc}(p_i))$.  Examples of Schweder and Spjøtvoll's p-value plot are included in the supplemental materials. 

@SchwederPlotsPvaluesEvaluate1982 give a brief (and rather informal) argument that the relationship between $1-p_i$ and $rank_{desc}(p_i)$ should be approximately linear "when [$p_i$] is not too small" and that, from left to right, "often, the plot will not show a clearcut break but rather a gradual bend" away from linearity.  Their argument concludes that "the slope of that straight line is an estimate of ... the number of true null hypotheses" in \P, and so the former can be used to estimate the latter [@SchwederPlotsPvaluesEvaluate1982 494].[^ss_replication]  Note that this means, as a method, Schweder and Spjøtvoll's p-value plot generally ignores "small" p-values, which roughly correspond to the statistically significant p-values.  They illustrate their method with example data and a straight line "drawn by visual fit" rather than regression analysis.  

[^ss_replication]: As this sentence indicates, Schweder and Spjøtvoll's p-value plot is not actually designed for the replication/meta-analysis setup that I introduced earlier in this section, that is, a setup in which the $p_i \in \P$ are produced by replicating a given study design corresponding to a fixed null hypothesis.  Instead, their setup assumes a collection of *independent* null hypotheses $H_t$, $t = 1, ..., T$, with a single study of each null hypothesis producing a single p-value $p_t$.  Their problem is to estimate the number of true null hypotheses $T_0 \leq T$.  This is another way in which Schweder and Spjøtvoll's p-value plot is distinct from both Young's p-value plot and the p-curve (see below), which are typically applied in the context of replications or meta-analysis examining a single underlying hypothesis. 

## Simonsohn, Nelson, and Simmons' p-curve ##

Young and collaborators have regularly echoed concerns about the replication crisis unfolding in social psychology and certain areas of biomedical research [for example, @YoungReliabilityEnvironmentalEpidemiology2019 50].  In particular, they appeal to concerns about p-hacking [@SimonsohnPcurveKeyFiledrawer2014].  Note that, other than Young's p-value plot, Young and collaborators have provided no specific[^phacking] empirical[^counting] evidence of p-hacking in environmental epidemiology, and to my knowledge no such evidence has been published [redacted]<!-- open science paper-->.  

[^phacking]: Young and collaborators do cite @HeadExtentConsequencesPHacking2015, which used text mining methods to examine p-values reported in "all Open Access papers available in the PubMed database," classified at the journal level into 22 disciplines.   @HeadExtentConsequencesPHacking2015 did not report the full size of their sample, but did include tens of thousands of p-values from "Medical and health sciences," which likely includes epidemiology but also other fields with very different methods, e.g., small-n animal model experiments and industry-funded clinical trials.  While their statistical tests did find evidence of p-hacking in "Medical and health sciences," they did not consider subfields.  Based on the methodological and funding diversity of the fields covered, and the lack of information about the distribution of subfields within their sample, we should be hesitant about drawing inferences to subfields.  

[^counting]: The first argument discussed in section @sec:reconstruction — that there are many possible alternative analyses of a given dataset, and so p-hacking might be possible — is not an empirical argument.  Except for @YoungCerealinducedGenderSelection2009 (which uses data from a nutrition study) and @YoungAirQualityAcute2017 and @YouPM2OzoneIndicators2018 (both of which work with datasets assembled by the authors), Young and collaborators do not conduct any analysis of any data when they make this argument.  Instead, the argument is purely combinatorial:  there are $k$ covariates available and so there are $2^k$ different subsets of covariates, each of which corresponds to a possible alternative model, and so on.  

Young and collaborators have frequently associated Young's p-value plot with a method developed to detect p-hacking, called a "p-curve" [@SimonsohnPcurveKeyFiledrawer2014; for a comparison of several methods to detect p-hacking, see @McShaneAdjustingPublicationBias2016].  The intuition behind the p-curve is that p-hacking will tend to produce an excess number of p-values "just below" the conventional $0.05$ threshold for statistical significance.  Formally, Simonsohn et al.'s p-curve first divides the interval $[0, 0.05]$ into 5 bins at the thresholds $0.01, 0.02, 0.03, 0.04, 0.05$, then calculates $N_b$, the number of p-values in bin $b$.  The p-curve is the graph $(b_t, N_b)$, where $b_t$ is the threshold for bin $b$.  The method then formally tests for p-hacking by applying statistical tests of the null hypothesis that the restricted distribution $P|_{p < 0.05}$ is uniform.  Examples of Simonsohn et al.'s p-curve using simulated data are included in the supplemental materials. 

Note that the p-curve is a histogram on the interval $[0, 0.5]$ with binwidth $0.01$, and that it only includes statistically significant p-values.  Thus the p-curve and Schweder and Spjøtvoll's p-value plot not only produce different kinds of plots using the same input data \P, but actually direct their attention to different — typically disjoint — subsets of \P.  The two kinds of plots cannot be equivalent to each other.  At no point have Young and collaborators acknowledged this fundamental difference between the two methods that they cite as support for their own method.  

## Young's p-value plot ##

For Young's p-value plot, let $rank_{asc}(p_i)$ be the (1-indexed) *ascending rank* of $p_i \in \P$, i.e., $rank_{asc}(p_i)$ is the number of p-values $p_j \in \P$ less than or equal to $p_i$.  The smallest p-value has ascending rank 1, and the largest p-value has ascending rank $N$.  Without loss of generality, if \P is already in ascending order $p_1 < p_2 < \cdots < p_N$, then $rank_{asc}(p_i) = i$.  And Young's p-value plot is the graph $(i, p_i)$.  

Statistically-minded readers might have already noted that Young's p-value plot is a rescaled QQ-plot of $\P$ against the uniform distribution, with the theoretical quantiles $q_i = \frac{i}{N} = \frac{rank_{asc}(p_i)}{N}$.  It is not equivalent to the other two plots, and so cannot be validated by references to them.  First, $rank_{desc}(p_i) = N - rank_{asc}(p_i) + 1$, and so for a fixed number of studies $N$ Young's p-value does have a 1-1 mathematical relationship to Schweder and Spjøtvoll's p-value plot.  In geometric terms, Young's p-value plot swaps the axes of Schweder and Spjøtvoll's p-value plot and reverses the direction of the ranking.  However, a regression line fit to Schweder and Spjøtvoll's p-value plot will not deterministically correspond to a regression line fit to Young's p-value plot.[^slopes]  In addition, the properties that Young and collaborators use in their analysis of their p-value plots do not correspond to the properties used by Schweder and Spjøtvoll (which, again, are justified with informal arguments rather than a formal analysis).  So, even if Schweder and Spjøtvoll's p-value plot can be considered validated for certain purposes (it should be clear that I'm skeptical on this point), this does not validate Young's p-value plot as used by Young and collaborators.  Second, Simonsohn, Nelson, and Simmons' p-curve constructs a histogram on a subset of p-values; this is a completely different construction from Young's p-value plot, and so citations to the former also do not validate the latter.  

[^slopes]: For an ordinary least-squares regression of $y$ against $x$, the slope of the fit regression line is $r s_x / s_y$, where $r$ is the correlation coefficient between $x$ and $y$ and the $s$ are the standard deviations.  So the slope of $x$ and $y$ (swapping the axes) is $r s_y / s_x$.  The ratio of the first slope to the reciprocal of the second slope is $r^2$.  This might suggest a deterministic relationship between the slopes of Schweder and Spjøtvoll's and Young's p-value plots.  However, $r$ is calculated from observations drawn from the random variables $X$ and $Y$, and so $r$ itself is an observation drawn from a random variable.  That is, the observed value of $r$ will vary between different iterations of the study.  So the relationship between the slopes of the two p-value plots will be noisy.  Scatterplots of regression slopes for the two plots, across all runs of the simulation, are included in the automatically reproduced analysis, illustrating the noise in this relationship.  *[put in supplement]*

Young and collaborators have given the following explanation of their analysis of Young's p-value plot:  "Evaluation of a p-value plot follows a logical path. Is the p-value plot homogeneous? If the points roughly [sic] on a 45-degree, they are homogeneous and consistent with randomness; a lessor slope with all points roughly on a line indicates a consistent effect even if some of the individual p-values are not considered statistically significant. If the effects differ, one from another, beyond chance, then the effects are heterogeneous" [@YoungReliabilityEnvironmentalEpidemiology2019 48].  

A "45-degree line" typically refers to the graph of an identity function or the line $y = x$, which forms a 45-degree angle with respect to both the x- and y-axes.  This interpretation makes sense for a QQ-plot of p-values against the uniform distribution on $[0, 1]$, where both axes are on the scale $[0, 1]$; here a slope of 1 indicates that the underlying distribution $P$ is uniformly distributed, which in turn indicates that the real effect is zero.  Strictly speaking, this interpretation does not make sense for Young's p-value plot, where the x-axis (rank) is on the scale $[1, N]$ and the y-axis (p-value) is on the scale $[0, 1]$.  I will assume that a "45-degree line" typically means that the slope of the equivalent QQ-plot is 1.  Note that Young and collaborators appear to only evaluate slopes visually; they do not report fitting regression models or using any other quantitative methods to measure slopes.  This point will be important in interpreting the simulation results.  

Young and collaborators have frequently identified a "hockey stick" pattern [@YoungReliabilityEnvironmentalEpidemiology2019; @YoungAmbientAirPollution2019; @YoungCombinedBackgroundInformation2019; @YoungEvaluationMetaanalysisAir2019], which has "small p-values to the lower left ... and then points ascending in a roughly 45-degree line"; here "45-degree line" seems to mean that the right-hand side of the plot is linear, even if it does not have a slope of 1 [@YoungEvaluationMetaanalysisAir2019 5].  Young and collaborators have taken this non-linear "hockey stick" pattern to indicate some combination of heterogeneous effects, p-hacking, and researcher misconduct.  On two occasions Young and collaborators appear to have formally tested non-linearity by comparing a linear and quadratic regression using an F-test [@YoungReliabilityEnvironmentalEpidemiology2019 50; @YoungCombinedBackgroundInformation2019 18].  More often they appear to have identified the "hockey stick" pattern by visual inspection alone.  

Young and collaborators do not appear to have validated the p-value plot method in any peer-reviewed studies, relying primarily on the references to the other two graphical methods.  @YoungCombinedBackgroundInformation2019 (8-9) did briefly report a simulation study.  In my notation, each run of this simulation had $N = 15$ p-values drawn from a condition in which the zero hypothesis $\delta = 0$ is true.  They conducted $NN = 10$ simulation runs, but did not report any aggregate or distributional statistics; instead they reported the smallest (ascending rank = 1) p-value and the p-value plot for each simulation run.  Critically, they did not report any simulations of cases where the real effect was non-zero or heterogeneous.  They also do not report making the simulation code available anywhere for checking reproducibility or extending their analysis.  


# Simulation study of Young's p-value plot # {#sec:simulation}

This section reports the design and results from a simulation study of the p-value plot.  

For readers who are not interested in the technical details of the simulation, the major results are as follows:  The p-value plot method can provide evidence for a zero effect, but only under certain circumstances and when used in certain ways.  The uses by Young and collaborators do not satisfy these conditions, and so the p-value plot does not provide evidence for any of their claims of a zero effect.  The p-value plot cannot — under any circumstances — provide evidence to support claims of statistical heterogeneity, p-hacking, publication bias, and the like.  And so the p-value plot does not support any such claims made by Young and collaborators.  In short, as used by Young and collaborators, the p-value plot does not provide evidence. 

This section proceeds as follows.  In the remainder of this introductory subsection, I specify the parameterization of the simulation and discuss four ways in which the plots are analyzed.  Section @sec:evidence discusses two quantifiable conceptions of evidence from the philosophy of science literature — severity and likelihood ratios — and explains how they are implemented in a reproducible way in the simulation.  Section @sec:reproducibility provides reproducibility information about the simulation, including URLs for viewing the automatically-reproduced analysis and downloading the complete source code for the simulation.  Section @sec:results reports the results of the simulation.  

Each run of the simulation comprises $N$ studies, collected together as though for a single meta-analysis.  To make the study parameters easy to understand, each study is based on a two-sample t-test.  Two samples, each of size $n$, are drawn from Gaussian distributions with means $\mu_1$ and $\mu_2 = \delta$, respectively, and common standard deviation $\sigma$.  (This data generating process is just slightly more complicated for the heterogeneous case; see below.)  $\delta$ is the same for all studies in the simulation run and, by setting $\mu_1 = 0$ and $\sigma = 1$, $\delta$ corresponds to the true effect size as measured by Cohen's $d = (\mu_2 - \mu_1) / \sigma$ [@SawilowskyNewEffectSize2009]. 

A *condition* of the simulation fixes the values of the other study parameters.  Conditions can be systematically varied to compare, e.g., different effect sizes, and multiple runs $NN$ in each condition allow us to analyze the statistical properties of the p-value plot within and across conditions.  For the primary analysis, 7 different effect sizes are used — corresponding to zero, very small, small, moderate, large, and very large effects [@SawilowskyNewEffectSize2009], and a "mixed" or heterogeneous condition.  All studies have the same sample size, $n = 26$.  Compared to the convention of 80% power, this sample size makes the studies severely underpowered to detect the very small (power 3%) and small effects (11%), somewhat underpowered for the moderate effect (42%), adequately powered for the large effect (81%), and overpowered for the very large effect (99%). 

Each condition is simulated with $NN = 500$ runs.  This setting should produce good estimates of central tendency and variation with just a few minutes of computation time (approximately 10 minutes on the service used to automatically reproduce the analysis).  Still, 500 runs is probably too few to produce highly precise estimates; so, for example, we should not finely discriminate between estimated p-values of $.051$ and $.049$.  The primary conditions examined in the current study are summarized in [@tbl:params].  

| parameter | meaning                             | value(s) |
|:----------|:------------------------------------|---------:|
| $\delta$  | real effect size                    | $0, .01, .2, .5, .8, 1.2, \{0, .8\}$ |
| $\sigma$  | s.d. of samples                     | 1        |
| $n$       | study sample size                   | 26       |
| $N$       | num. studies in each run            | 20       |
| $NN$      | num. runs for each condition        | 500      |

Table: **Parameter values used in the current simulation study.**  The real effect size of $\{0, .8\}$ indicates the "mixed" or heterogeneous condition, in which half the population have no response and half the population have a strong response. {#tbl:params}

In the "mixed" or heterogeneous condition, subsets of the population have different responses to the intervention or exposure.  I use a $\{0, .8\}$ mixture, meaning that one subpopulation has a zero effect or no response to the intervention and the other has a strong response of .8.  These two quite different values were chosen to create a mixture that is less likely to look like a homogeneous case; a condition with a $\{.3, .5\}$ mixture would be difficult to distinguish from a homogeneous .4 condition.  

When given this kind of mixture, in the simulation, an individual study draws its sample from one of the subpopulations, selected uniformly at random.  So, in expectation, half of the studies will find a zero effect and half a strong effect, though in any given set of studies there will be variation in the ratios of the two subpopulations.  

For simplicity, the simulation currently does not support a continuous mixture, that is, where the real effect for a given study is drawn from some continuous distribution.  Such an extension of the simulation could be implemented in future work.  I expect that, if the p-value plot is unable to distinguish the discrete $\{0, .8\}$ mixture from homogeneous mixtures, then it would be even less able to distinguish a continuous mixture.  

After generating the data, it is straightforward to construct the p-value plot.  I then analyze the plot four ways.  First, graphs can be assessed visually, which appears to be the primary approach taken by Young and collaborators.  I draw a small sample of simulation runs, uniformly at random, and display their plots for visual inspection.  Of course, visual assessment cannot be scaled up to thousands of replications or reproduced computationally.  

Second, to assess "gaps" in the plot, I calculate the largest difference in consecutive p-values $\max(p_{i+1} - p_i)$.  I classify a p-value plot as "gappy" if this largest difference is greater than a threshold value, .125.  That is, if there is at least one visual gap of at least 12.5% in the p-value plot, the plot is considered "gappy."  While this threshold is arbitrary, I think it reasonably captures the sense of a visual "discontinuity" in the sequence of p-values.[^gaps]

[^gaps]: An anonymous reviewer argues that this operationalization is inappropriate, because only the "gaps" around $p=.05$ matter.  But this is not the way Young and collaborators interpret the "gaps."  @YoungEvaluationMetaanalysisAir2019 highlight a gap from about 0.4 to 0.8, attributing this "to chance or a file drawer effect" (5). 

Third, slopes can be calculated using a simple univariate linear regression.  If the slope of the QQ-plot is approximately 1, this indicates that the underlying distribution of p-values $P$ is uniform, which in turn indicates that the zero hypothesis is true.  Because the slope of the p-value plot is $N$ times the slope of the QQ-plot, analysis of the QQ-plot is simpler than analyzing the p-value plot directly.  

After fitting a linear regression to the QQ-plot, I assess its slope in several ways, to judge whether it is "approximately 1."  First, I consider simply whether the slope is in the range $1\pm0.1$.  This interval is arbitrary, but I don't think it is an unreasonable operationalization of "approximately 1."  Next, I apply a t-test against the null hypothesis that the slope is exactly 1.  I also apply an equivalence test, the TOST procedure [@LakensEquivalenceTestingPsychological2018], against the null hypothesis that the slope is "substantially different" from 1.  The TOST (two one-sided test) procedure first requires defining an equivalence interval; here I use the interval $1\pm0.1$ again, as an arbitrary but not unreasonable range of values "approximately 1."  The procedure then conducts two one-sided t-tests, first against the null hypothesis that the value of interest is greater than the left-hand bound (that is, that the slope is greater than 1.1) and second against the null hypothesis that the value of interest is less than the right-hand bound (that is, the slope is less than .9).  The observed estimate passes the TOST test if and only if both one-sided t-tests are statistically significant.  If the estimate passes, we infer that the true value is within the equivalence interval, that is, "approximately 1." Finally, I use a Kolmogorov-Smirnov test (KS-test) to compare the observed set of p-values \P to the uniform distribution.  I use the conventional 0.05 threshold for statistical significance for all three tests.  When the estimate is not statistically significant, the tests as implemented in the simulation accept the null hypothesis; while strictly problematic, this approach simplifies the presentation and analysis of results and aligns with the way hypothesis tests are used in practice.  Because we are interested in these tests independently — as though the other analyses of the slope were not conducted — there is no need to correct for multiple comparisons.  

The fourth analytical approach is to evaluate the linearity of the plot.  Young and collaborators take non-linearity as evidence of some combination of heterogeneity, p-hacking, and researcher misconduct.  Linearity of a plot can be tested by fitting two regression models to its graphs — one linear, one quadratic — and selecting the model with the better fit.  I use two standard methods of model selection to compare regression models, AIC and an F-test.  I use the conventional 0.05 threshold for statistical significance in the F-test, and accept the null (linear model) when the F-test is not significant.  While AIC is a random variable[^random], and could be compared more rigorously using a testing framework, I follow the way AIC is used in practice and simply compare the point estimates.  As with slope, for simplicity I analyze the QQ-plot rather than the p-value plot directly.  

[^random]: That is, AIC is calculated from observations drawn from a random variable (the data collected in one iteration of a study), and so the observed value of AIC will vary between rounds of observation (iterations of the study).  In this respect AIC is no different from, say, the sample mean, and could be treated in the same way:  one could derive (or estimate) the sampling distribution of AIC, then use this to construct confidence intervals around the observed AIC, test the hypothesis that the AICes of two fitted models are different, and so on.  


## Measuring Evidence ## {#sec:evidence}

Again, Young and collaborators have interpreted the p-value plot as providing evidence for zero effects, heterogeneity, and p-hacking in the epidemiological literature on air pollution hazards.  Within the simulation, zero effects correspond to $\delta = 0$ and heterogeneity is represented with the $\delta = \{0, 0.8\}$ condition.  This allows us to quantitatively assess evidence for these conditions as hypotheses.  The simulation does not have a way to represent p-hacking, publication bias, or researcher misconduct; however, this means that all conditions represent cases in which these factors are false.  

I use two quantifiable conceptions of evidence that have been widely discussed in the philosophy of science literature, severity and likehood ratios.  These two conceptions of evidence are typically treated as philosophical rivals and associated with different philosophies of probability and statistics.  Using both allows me to remain agnostic to these disagreements.  

Attempts to quantify evidence — and especially the interpretation of quantified evidence using thresholds, as I do below — may remind some readers of the literature on inductive risk.  Consider the choice to set the threshold for a "severe test" at .05, .01, or .005 [compare @BenjaminRedefineStatisticalSignificance2018].  Setting this threshold at different values will lead to different patterns of acceptance and rejection of claims, which will have downstream non-epistemic consequences for, say, air pollution regulation.  There is widespread agreement among philosophers of science today that these non-epistemic consequences should be taken into account when the thresholds are set upstream [@DouglasSciencePolicyValuefree2009; @ElliottExploringInductiveRisk2017].  At a minimum, this suggests that I should not adopt conventional thresholds (as I do below) without considering whether they are appropriate given the downstream non-epistemic consequences at hand.  

However, in an analysis of climate politics, @HicksScientificControversiesProxy2017 observes that there is disagreement about the relative importance of different kinds of downstream non-epistemic consequences.  Air pollution politics is marked by similar disagreement.  Thus, applying inductive risk considerations here would require an ethical argument for the relative importance of different kinds of consequences.  Given space constraints, such an argument is beyond the scope of this paper.  [redacted]  <!-- (I give such arguments in @HicksNewDirectionScience2014, @HicksInductiveRiskRegulatory2018, @FernandezPintoLegitimizingValuesRegulatory2019, and @HicksWhenVirtuesAre.) -->  One advantage of conventions is that they can be somewhat acceptable from a wide variety of perspectives on a controversy.  I therefore adopt the conventions, recognizing that a more adequate analysis would address inductive risks directly.  

### Severity ###

The severity conception of evidence is associated with philosopher of science Deborah Mayo's reconceptualization of frequentist hypothesis testing [@MayoErrorGrowthExperimental1996; @MayoStatisticalInferenceSevere2018].[^blurb]  In its most recent form, the severity conception of evidence involves two "severity requirements":  

[^blurb]: Young provided a positive blurb for @MayoStatisticalInferenceSevere2018, stating that "Her severity requirement [sic] demands that the scientist provide a sharp question and related data. Absent that, the observer should withhold judgment or outright reject." 

- *Severity (weak)*: One does not have evidence for a claim if nothing has been done to rule out ways the claim may be false. If data $x$ agree with a claim $C$ but the method used is practically guaranteed to find such agreement, and had little or no capability of finding flaws with $C$ even if they exist, then we have bad evidence, no test (BENT). [@MayoStatisticalInferenceSevere2018 5]
- *Severity (strong)*: We have evidence for a claim $C$ just to the extent it survives a stringent scrutiny. If $C$ passes a test that was highly capable of finding flaws or discrepancies from $C$, and yet none or few are found, then the passing result, $x$, is evidence for $C$. [@MayoStatisticalInferenceSevere2018 14]

On this conception of evidence, a test or analytical method $T$ with observed output $t$ can provide evidence supporting a hypothesis $H$ only if $T$ would have given a different output if $H$ were false.  Hypothesis testing assesses this counterfactual condition in the form of a p-value using a mathematical model of the data-generating process: $p = pr(T = t | \lnot H) = L(\lnot H; T = t)$, where the role of $\lnot H$ is played by a null hypothesis $H_0$, which often states that some quantity of interest is zero.[^nullzero]  A "small" p-value indicates that the counterfactual is probably true, that is, if $H$ were false then $T$ would probably have given a different output.  On the other hand, a "large" p-value indicates that the test "is practically guaranteed" to produce this output, and so in this case by the weak severity principle "we have bad evidence, no test." 

[^nullzero]: Throughout this paper, I am distinguishing a *zero hypothesis* — that some effect is zero — from a *null hypothesis* — the alternate or rival hypothesis used to calculate a p-value.  Null hypotheses can claim that some quantity of interest is equal to a non-zero value.  For example, if we are using the slope of a QQ-plot to test whether a set of p-values is not uniformly distributed, the null hypothesis might that the slope is 1. 

Severity can also be evaluated informally when a p-value cannot be calculated.  Consider visual features of plots, such as the "hockey stick" pattern that Young and collaborators interpret as evidence of heterogeneity.  It's not clear how to quantitatively determine whether this visual pattern is present in a given plot.  However, if this pattern is (qualitatively) common in homogeneous cases, then the weak severity principle implies that this visual pattern does not provide evidence of heterogeneity.  

The severity conception of evidence can be applied to Young and collaborator's skeptical claims about air pollution hazards as follows.  The claims $H$ are the zero hypothesis $\delta = 0$ and mixture hypothesis $\delta = \{0, .8\}$.  The method or test $T$ is the p-value plot, analyzed visually or in terms of slopes or linearity; the outputs $t$ that I will consider are (i) the "hockey stick" pattern for the visual analysis; (ii) "gaps" in the plot, based on a visual analysis and the size of the largest difference in consecutive p-values; (iii) slope of approximately 1 (on the QQ-plot), assessed based on whether the slope (as estimated using linear regression) falls into the range $1\pm0.1$, non-statistically significant results for a t-test against the null hypothesis that the slope is exactly 1, statistically significant results for TOST test against the null that the slope falls outside the range $1\pm0.1$, and non-statistically significant results for the KS-test of uniformity; and (iv) non-linearity inferences using AIC and the F-test.  

[@Tbl:outputs] summarizes the outputs examined in the current study.  

|     | output            | determined using      | taken as evidence for |
|----:|:------------------|:----------------------|:----------------------|
|   i | "hockey stick"    | visual inspection     | mixed effect          |
|  ii | "gaps"            | visual inspection     | p-hacking or other problems |
|     |                   | largest gap $> .125$  ||
| iii | slope $\approx$ 1 | range $1\pm0.1$       | zero effect           |
|     |                   | T-test not stat. sig. ||
|     |                   | TOST test stat. sig.  || 
|     |                   | KS-test not stat. sig.|| 
|  iv | non-linearity     | AIC: quadratic        | mixed effect
|     |                   | F-test: stat. sig.    ||

Table: **Outputs of the p-value plot examined using the simulation.**  "Outputs" are features of plots that Young and collaborators take as evidence for critical assessments of air pollution epidemiological studies.  The "determined using" column indicates how these outputs are identified as present/absent in the current study.  {#tbl:outputs}

To generate p-values (or informally assess severity for the visual analysis), we need to specify a null hypothesis for $\lnot H$.  I will use each of the following:  

a. very small: $\delta = .01$
b. small effect: $\delta = 0.2$
c. moderate effect: $\delta = 0.5$
d. strong effect: $\delta = 0.8$
e. very strong effect: $\delta = 1.2$
f. greater than zero: $a \vee \cdots \vee e$
g. mixed effect: $\delta = \{0, .8\}$ for $H: \delta = 0$ and vice-versa (that is, the other skeptical hypothesis)
h. any other effect: $f \vee g$ (any of the non-zero effects or the other skeptical hypothesis)

In each case, insofar as the p-value is "large" $p > .05$, the simulation results indicate that this test output is common in the null hypothesis case, and so the weak severity criterion implies that the p-value plot does not provide evidence to support the skeptical claim made by Young and collaborators.  

### Likelihood ###

The likelihood conception of evidence is not strongly associated with any one statistician or philosopher of science, though it can be associated with one approach to Bayesian statistics [@KassBayesFactors1995; @RomeijnPhilosophyStatistics2017].  

Formally, the likelihood conception of evidence compares two rival hypotheses $H_1$ and $H_2$ using some data $d$.  The likelihood ratio is defined as
$$ K(H_1, H_2; d) = \frac{L(H_1; d)}{L(H_2; d)} = \frac{pr(d | H_1)}{pr(d | H_2)}. $$
If $K > 1$, then the evidence favors $H_1$; and $K < 1$ then the evidence favors $H_2$.  Sometimes $\log K$ is used to create symmetry between $H_1$ and $H_2$.  On one common interpretive scale, $\left|\log_{10} K\right| < 0.5$ is "not worth more than a bare mention," not supporting either hypothesis; $0.5 < \left|\log_{10} K\right| < 1$ is "substantial"; $1 < \left|\log_{10} K\right| < 2$ is "strong"; and $2 < \left|\log_{10} K\right|$ is "decisive" [@KassBayesFactors1995]. 

To apply the likelihood conception of evidence to Young and collaborators' skeptical claims about air pollution, $H_1$ will be the zero or mixture hypothesis, the rival hypothesis $H_2$ will be the hypotheses (a-h), and the data $d$ will be the analysis outputs (i-iv).  (For simplicity, the same dichotomous frequentist test outputs are used, e.g., statistically significant or not, rather than continuous-valued likelihoodist or Bayesian alternatives.)  In each case, insofar as $K < 0.5$, this implies that the p-value plot does not provide evidence to support the zero or mixture hypotheses.  

## Reproducibility ## {#sec:reproducibility}

The simulation, analysis, and outputs (figures and tables) are publicly available and automatically reproduced.  Code is available at [redacted] and the automatically reproduced analysis can be viewed at [redacted].  
<!--<https://github.com/dhicks/p_curve> <https://dhicks.github.io/p_curve/>-->

The simulation and analysis were both written in R version 4.0.0 [@RCoreTeamLanguageEnvironmentStatistical2018] and make extensive use of the `tidyverse` suite of packages, version 1.3.0 [@WickhamTidyverseEasilyInstall2019].  The TOSTER package version 0.3.4 [@LakensTOSTERTwoOneSided2018] was used to conduct the TOST analysis.  Because the software on the virtual machine used to automatically reproduce the analysis is updated each time the analysis is re-run, software versions reported online may be different from those reported here. 



## Simulation results ## {#sec:results}

Figures and tables included in the automatically reproduced analysis validate the simulation, showing that both the individual studies and meta-analyses (average within each simulation run) produce unbiased estimates of the real effect size $\delta$ (except for the mixed case, where there isn't a unique real effect size; though the meta-analysis estimate is 0.4, the average of the two real effects 0 and 0.8).  


### Visual analyses ###

[@Fig:samples_young] shows 25 examples of the p-value plot across the seven conditions, and [@fig:composite_young] shows the p-value plot across all runs of the simulation.  (See the supplemental material for examples of Schweder and Spjøtvoll's and Simonsohn et al.'s plots.)  

![**Examples of the p-value plot.**  Drawn at random from the simulation results.  Rows and colors correspond to conditions or real effects ($\delta$), from zero (0) to very strong (1.2) and a mixed condition $\delta = \{0.0, 0.8\}$.  Columns correspond to indices for the simulation runs that produced these results, and are not meaningful.  Each point corresponds to a single p-value in the meta-analysis (simulation run); the x-axis is the ascending rank of the p-value in the set \P, and the y-axis is the p-value itself.](fig_1_samples_young.png){ #fig:samples_young width=6in height=4in }

![**Composite of the p-value plot.**  Each individual line is the p-value plot for a single run of the simulation; all simulation runs are shown here.  Panels correspond to conditions or real effects ($\delta$), from zero (0) to strong (1.2) and a mixed condition $\delta = \{0.0, 0.6\}$.](fig_2_young_composite.png){ #fig:composite_young width=6in height=4in }

As preliminary observations, note that there are substantial qualitative differences within effect sizes as well as similarity across consecutive effect sizes.  Except for the very large (overpowered) effect size, there tends to be both statistically significant and insignificant p-values.  Larger effect sizes have more statistically significant results, resulting in a series of small p-values that gradually bend up.  However, the top row of [@fig:samples_young] and the first panels of [@fig:composite_young] indicates that even zero effects can look nonlinear.  Comparing the composite plots ([@fig:composite_young]) for the mixed condition and the moderate effects condition, on average the mixed condition tends to produce a sharper bend upwards.  However, [@fig:samples_young] indicates that an individual moderate effects plot can have a sharp bend (index 290) and a mixed effects plot can have a gradual bend (index 66, second column from the left).  

I focus on two visual patterns that Young and collaborators frequently discuss in their critiques of air pollution epidemiology:  (i) the "hockey stick" pattern; (ii) "gaps" in the plot.  

Young and collaborators take the "hockey stick" pattern to be evidence of heterogeneity.  The "hockey stick" comprises a more-or-less flat series of small p-values on the left (supposedly corresponding to the mixture component with a real effect) and a second series of steeply increasing p-values on the right (supposedly corresponding to the mixture component with zero effect).  This pattern is clearly visible in all of the example plots for the moderate, strong, very strong, and mixed effects (bottom four rows).  Because the "hockey stick" pattern appears in plots where there is only a single homogeneous effect, the weak severity criterion implies that *the hockey stick pattern does not provide evidence of a mixed or heterogeneous case*.  

Comparing panels in [@fig:composite_young], it seems reasonable to me to say that *on average* the mixed effect curve starts to visually bend up somewhat earlier — around study 7-12 — than the moderate and strong effects curves — around study 15 for moderate effects and study 18 for strong effects.  However, as [@fig:samples_young] indicates, any individual plot can bend up somewhat "earlier" or "later" than average.  Even if simple visual inspection could produce an intersubjectively precise judgement of when a given plot starts to bend, it would take further analysis to determine whether the "bend point" of a given plot is typical or unusual, relative to a hypothesized effect size.  Recall the first sentence of Mayo's definition of the weak severity criterion:  "One does not have evidence for a claim if nothing has been done to rule out ways the claim may be false" [@MayoStatisticalInferenceSevere2018 5].  Without an analysis of the location of "bend points" across different conditions, the antecedent in this version of weak severity is true, and so again the hockey stick pattern does not provide evidence of a mixed or hetergeneous case. 

Young and collaborators take visual "gaps" in the plot to be evidence of p-hacking, publication bias, or other questionable researcher practices [@YoungAssessingGeographicHeterogeneity2013; @YoungCombinedBackgroundInformation2019; @YoungEvaluationMetaanalysisAir2019].  These gaps are quite common in [@fig:samples_young], across all conditions except the very strong effect (where almost all values are below 0.05).  Note that the simulation does not include p-hacking, publication bias, or other questionable researcher practices.  Thus the weak severity criterion implies that *gaps in the plot do not provide evidence of p-hacking, publication bias, or other questionable researcher practices*.  



### Computationally reproducible analyses ###

Three outputs of the analysis of the p-value plot can be quantified and reproduced computationally:  (ii) "gaps" larger than .125 or 12.5 percentage points, (iii) a slope of approximately 1 (corresponding to the "45-degree line"), and (iv) non-linearity.  Again, Young and collaborators take visual "gaps" in the plot to be evidence of p-hacking, publication bias, or other questionable researcher practices [@YoungAssessingGeographicHeterogeneity2013; @YoungCombinedBackgroundInformation2019; @YoungEvaluationMetaanalysisAir2019]; a "45-degree line" as evidence for zero effect [@YoungCerealinducedGenderSelection2009; @YouPM2OzoneIndicators2018; @YoungCombinedBackgroundInformation2019; @YoungReExtendedMortality2020]; and non-linearity as evidence for heterogeneity or mixed effects 
[@YoungReliabilityEnvironmentalEpidemiology2019; @YoungAmbientAirPollution2019; @YoungCombinedBackgroundInformation2019; @YoungEvaluationMetaanalysisAir2019].  For (iii) and (iv) I use the QQ-plot corresponding to the p-value plot, assessing a slope of 1 in four ways and non-linearity in two ways, as discussed above.  

#### Severity analysis

@Fig:evidence_severity show the results of the severity analysis as p-values; see the supplemental materials for a table version of these results.  By the weak severity criterion, when the results of the severity analysis are greater than .05 (above the dashed line), the test output does not provide evidence in support of the target hypothesis.  

![**Results of the severity analysis for outputs (ii) gaps in the plot, (iii) slope of 1, and (iv) non-linearity.**  Severity analysis results are reported as p-values:  small values (conventionally $< .05$) indicate a severe test with respect to the null hypothesis.  Panels correspond to null hypotheses, and y-axis values correspond to the severity assessment (as a p-value) for the output with respect to the given null hypothesis.  The dashed line indicates $p = 0.05$; *points below this line indicate severe tests*.](fig_3_evidence_severity.png){#fig:evidence_severity width=6in height=4in}

Unspecified problems of p-hacking, publication bias, or other questionable researcher practices are supposedly supported by gaps in the plot.  The p-value for the presence of these gaps (output ii-gap) is greater than .25 for every null hypothesis except the very strong effect, indicating that gaps are quite common.  In all conditions except the large and very large effects, the majority of p-value plots are "gappy."  Thus *gaps in the plot do not provide evidence of p-hacking, publication bias, or other questionable researcher practices.*  

The zero effect hypothesis is supposedly supported by a slope of approximately 1 on the QQ-plot.  All methods are severe against large and very large effects; only the TOST and KS tests are severe against moderate effects; and no methods are severe against very small or small effects.  The TOST test also may be severe against the greater-than-zero and non-zero hypotheses.  *So the "45-degree line" may or may not provide evidence for zero effects, depending on the particular null hypothesis being tested and particular test used.*  

This means that, in order to determine whether the "45-degree line" provides evidence for zero effects, the analyst must be explicit about both the test method used and the null hypothesis being tested.  A slope of .92 or statistically significant TOST test, say, will not provide evidence if the null hypothesis is a very small effect.  In addition, while these results recommend using the TOST test, this test requires an explicit equivalence interval, the range of values that are "approximately 1."  But Young and collaborators are not explicit in any of these requisite ways. Indeed, they never report calculating a slope, using a regression line, or other method, much less conducting some further analysis of that slope.  Instead, they present the p-value plot, state some version of the principle that "If the points fall on a 45-degree line, then the results are consistent with randomness" [@YoungCombinedBackgroundInformation2019 194], and move directly to some version of the conclusion that "The p-value plot for these data is consistent with pure randomness" [@YouPM2OzoneIndicators2018 193; compare @YoungCerealinducedGenderSelection2009; @YoungCombinedBackgroundInformation2019; @YoungReExtendedMortality2020]. At best, this means that the evidentiary value of their analysis is ambiguous.  We would need to know more in order to judge whether they have produced evidence to support a zero effect.  However, I take it that the lack of detail is a good reason to think that they have simply assessed the slope visually.  A visual assessment will be even less sensitive than calculating a slope and determining whether it is in the range $1\pm.1$, which can only provide evidence against a relatively large null hypothesis.  *So in cases where a small effect is a live possibility, insofar as Young and collaborators are relying on visual judgment, the "45-degree line" does not provide evidence of a zero effect.*  As I will discuss below, very small effects are common in air pollution epidemiology. 

The mixed-effect hypothesis, or heterogeneity, is supposedly supported by non-linearity.  Two quantified versions of this output are examined here, comparing linear and quadratic regressions using AIC (iv-AIC) and an F-test (iv-F).  The AIC and F-test evaluations of non-linearity do not provide severe tests against any of the alternative conditions.  That is, *neither of the tests of linearity considered here provide evidence for heterogeneity*.  



#### Likelihood analysis

@Fig:evidence_likelihood_zero and @fig:evidence_likelihood_mix show the results of the likelihood analysis; see the supplemental materials for a tables and interactive versions of these results.  Log likelihood ratios are reported, so results above $0.5$ support $H_1$ and results below $-0.5$ support $H_2$.  So, by the weak severity criterion, when the results of the likelihood analysis are $< 0.5$, the test output does not provide evidence supporting the target hypothesis of zero or mixed effect.  

![**Results of the likelihood analysis for $H_1: \delta = 0$.**  Each point gives the log likelihood ratio for $H_1$ vs. rival hypothesis, given an output.  Each panel represents one comparison of $H_1$ against a rival hypothesis $H_2$.  Position on the y-axis indicates the strength of the evidence that the output provides to the hypotheses:  *greater values indicate more support for $H_1$ over $H_2$*.  (Points at the plot margins have infinite value due to division by zero.)  Shaded regions indicate the degree of support for one hypothesis against the other, in order from lightest to darkest:  none, "substantial," "strong," "decisive."  An interactive version of this plot is included in the automatic reproduction of the analysis for this paper.](fig_4_evidence_likelihood_zero.png){#fig:evidence_likelihood_zero  width=6in height=4in }

![**Results of the likelihood analysis for $H_1: \delta$ is mixed.**  Interpretation is the same as @fig:evidence_likelihood_zero.](fig_5_evidence_likelihood_mix.png){#fig:evidence_likelihood_mix  width=6in height=4in }

For "gaps" in the plot, calculating the likelihood ratio would require simulation conditions that included p-hacking and other questionable research practices.  Because the simulation does not currently support these kinds of conditions, likelihood analysis cannot be used for this output.  
 
The zero effect hypothesis is supposedly supported by a slope of approximately 1.  All four methods provide "decisive" support for a zero effect against strong and very strong effects, and all except the T-test provide "substantial" or better support against moderate effects.  The TOST approach provides stronger or equally strong evidence, compared to the other approaches, across all of the rival hypotheses.  When the rival hypothesis includes small or very small effects, other approaches either do not provide evidence to support zero effects.  So, as with the severity analysis, *whether and to what degree the "45-degree line" might provide evidence for zero effects depends on the choice of rival hypothesis and analytical approach used*. In addition, and again as in the severity analysis, the visual assessment apparently used by Young and collaborators will provide weaker evidence than the range test, which does not provide evidence against rivals that include small effects.  *So, against rival hypotheses that include small effects, insofar as Young and collaborators are relying on visual judgment, the "45-degree line" does not provide evidence of a zero effect.* 

For the mixed effect hypothesis, all of the points for both AIC and the F-test are in the "not worth mentioning" or no evidence range, and so *neither method provides evidence to support heterogeneity*.  This is the same conclusion reached by the severity analysis. 



# Summary and conclusion ## {#sec:summary}

This simulation analysis finds that the p-value plot does not provide evidence for heterogeneity or p-hacking based on the "hockey stick" shape, "gaps" in the plot, or AIC or F-tests of non-linearity.  The method can provide evidence for zero effects based on a slope of 1, depending on what rival or null hypothesis is considered and how the plot is analyzed.  In general, producing evidence for zero effects against small effects requires using the TOST approach; visual inspection alone will not be sufficient.  This approach requires setting an explicit range of values within which the slope is considered "approximately 1."  

In the meta-analyses criticized by Young and collaborators, the estimated short-term effects of air pollution are small or very small on a relative risk scale. For example,   @NawrotPublicHealthImportance2011 estimated the effect for air pollution (increase of 10 $\mu g/m^3$ PM$_{10}$) on non-fatal myocardial infarction to be 1.02 (95% CI 1.01–1.02); the point estimates for six pollutants reported by @MustaficMainAirPollutants2012 (increase of 10 $\mu g/m^3$ for all except carbon monoxide) were all in the range 1.003-1.048; @LiuAmbientParticulateAir2019 estimated effects for PM$_{10}$ (increase of 10 $\mu g/m^3$) on all-cause mortality of 1.044 (95% CI 1.039-1.050); and @OrellanoShorttermExposureParticulate2020 estimated effects for PM$_{2.5}$ on all-cause morality of 1.0065 (95% CI 1.0044–1.0086).  A precise conversion from risk ratios to Cohen's $d$ is beyond the scope of this paper; however, using a rule of thumb that the risk ratio is approximately equal to the log odds when the outcome is rare [@VieraOddsRatiosRisk2008] and the conversion factor $\sqrt{3}/\pi$ between the log odds and Cohen's $d$, a risk ratio of 1.05 is roughly equivalent to $d=0.03$, which is only slightly larger than the very small effect condition examined here.[^power]  This indicates that *visual inspection of the p-value plot alone is incapable of producing evidence against very small epidemiological effects of air pollution*.  

[^power]: A supplemental analysis in the automatically reproduced analysis document examines conditions with a very small real effect sizes of $\delta = 0.05$ and varying power to detect these very small effects.  In severity terms, no analytical approach was capable of providing evidence when the primary studies were severely underpowered; the TOST approach was capable of providing evidence for a zero effect when the primary studies were moderately underpowered; and all approaches were capable of providing evidence for a zero effect when the primary studies were adequately powered.  Young and collaborators do not report a power analysis of any of the meta-analyses or primary studies that they criticize. 

More often, Young and collaborators have claimed to find evidence of heterogeneity, p-hacking, and publication bias [@YoungReliabilityEnvironmentalEpidemiology2019; @YoungAmbientAirPollution2019; @YoungEvaluationMetaanalysisAir2019].  The simulation results indicate that the p-value plot is incapable of providing evidence for any of these claims, using either visual inspection or either of the quantitative approaches examined here.  The features that Young and collaborators point to — the "hockey stick" shape, "gaps," non-linearity — are readily produced by moderate and stronger effects, and can even appear in zero and very small effect conditions.  

All together, Young and collaborators have repeatedly drawn conclusions that their p-value plot method cannot support. 

