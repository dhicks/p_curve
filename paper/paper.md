---
title: "Young's p-value plot does not provide evidence against air pollution hazards"
#author: "Daniel J. Hicks"
abstract: >
  S. Stanley Young and collaborators have criticized epidemiological studies and meta-analyses of air pollution hazards using a graphical method they call a p-value plot, claiming to find zero effects, heterogeneity, and p-hacking.  However, the p-value method has not been validated in a peer-reviewed publication. The aim of this study was to investigate the statistical and evidentiary properties of this method. A simulation approach was chosen to create studies and meta-analyses with known real effects $\delta$ and automatically analyze them using the p-value plot method. Two quantifiable conceptions of evidence were selected from the philosophy of science literature to assess the evidentiary value of the plot. The simulation and analysis is publicly available and automatically reproduced. 500 simulation runs were conducted across each of 5 real effect conditions, ranging from zero effects $\delta = 0$ to moderate-strong effects $\delta = 0.6$ and a heterogeneous condition. Young's p-value plot did not provide evidence for heterogeneity or p-hacking with respect to any condition. Depending on the exact rival hypothesis and analytical strategy, Young's p-value plot can provide evidence of zero effects. Young and collaborators most often use the p-value plot method to claim heterogeneity and p-hacking in the air pollution epidemiological literature. Young's p-value plot does not provide evidence to support these criticisms.
bibliography: Young.bib
header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
tblPrefix:
  - "table"
  - "tables"
---

\renewcommand{\P}{\ensuremath{\mathbb{P}}}
\newcommand{\SchSp}{Schweder and Spjøtvoll}

# Introduction #

In numerous recent papers [@YoungCerealinducedGenderSelection2009; @YoungAssessingGeographicHeterogeneity2013; @YouPM2OzoneIndicators2018; @YoungReliabilityEnvironmentalEpidemiology2019; @YoungAmbientAirPollution2019; @YoungCombinedBackgroundInformation2019; @YoungEvaluationMetaanalysisAir2019; @YoungReExtendedMortality2020; @YoungPM2AllcauseMortality2020], statistician S. Stanley Young and collaborators have criticized epidemiological studies and meta-analyses, especially those that find hazards associated with particulate matter (PM) exposure.  Young and collaborators have used a graphical method they call a p-value plot, claiming that this method reveals zero effects, heterogeneity, and p-hacking in the primary air pollution literature and so that meta-analyses are unreliable.  

Young and collaborators have drawn strong conclusions using their p-value plot method, claiming that "causality of PM10/PM2.5 on heart attacks is not supported" [@YoungReliabilityEnvironmentalEpidemiology2019] and "There is no convincing evidence of an effect of PM2.5 on all-cause mortality" [@YoungPM2AllcauseMortality2020]; and Young has advocated that "regulation of PM2.5 should be abandoned altogether" [@YoungSuggestionsEPA2017].  Perhaps because most of the papers using the p-value plot method were published in 2019, they have not been highly cited.  But a slightly older paper by Young and colleagues [@YoungAirQualityAcute2017] was cited in the scientific review of US EPA's Ozone Integrated Science Assessment [@USEPACleanAirScientificAdvisoryCommitteeReviewEPAIntegrated2020, p.D-50].  And Young became a member of US EPA's Science Advisory Board in 2017 [@USEPAMembersScienceAdvisory2017]. 

However, Young and collaborators have provided only a minimal analysis of the statistical properties of the p-value plot method, in a set of notes that has been made available on the arXiv, an open paper repository, but has not undergone peer review [@YoungCombinedBackgroundInformation2019].  They have sometimes attempted to justify their method by citing plots of p-values developed by other authors [@SchwederPlotsPvaluesEvaluate1982; @SimonsohnPcurveKeyFiledrawer2014].  But these other plots are designed to answer different questions and have different visual and statistical properties. 

Therefore, the aim of the current paper is to formally evaluate the statistical properties and evidentiary value of the p-value plot method used by Young and collaborators, before it can influence regulatory decisionmaking.  For this purpose, numerical simulations offer a number of advantages over exact, purely formal methods.  First, for the target audience of this paper, well-written R code is likely to be more accessible than a purely formal derivation of the statistical properties of meta-analyses.  This makes it easier for readers to scrutinize the details of my analysis.  Second, the simulation is modular, making it more extensible or modifiable by readers than a formal analysis.  The simulated data generating process and its analysis can be modified by simply rewriting two short functions and re-running the simulation.  Any formal analysis of a different data-generating process might require a completely different proof strategy.  Third, it may be algebraically intractable to give formal characterizations of constructions such as plots except in the limit (as sample size and/or number of studies goes to infinity).  Simulations can produce good numerical estimates in a few minutes.  

Based on the results of this simulation analysis, I find that the p-value plot method does not have the statistical and evidentiary properties required to support the criticisms leveled by Young and collaborators.  So it cannot provide evidence to support the criticisms made by Young and collaborators.  



# Reconstructing Young's Methods #

Young and collaborators have made three kinds of arguments criticizing air pollution epidemiology.  The most common argument first observes that analysts of complex data sets must make a number of decisions about which hypotheses to investigate and how to construct models; Young and collaborators then relate the large number of possible combinations of decisions to the traditional frequentist problem of correcting for multiple comparisons [@YoungCerealinducedGenderSelection2009; @YoungDemingDataObservational2011; @YoungReModelingAssociation2013; @PeaceReliabilityNutritionalMetaanalysis2017; @YoungAirQualityEnvironmental2017; @YoungAirQualityAcute2017; @YouPM2OzoneIndicators2018; @YoungAirPollutionMortality2018; @YoungReliabilityEnvironmentalEpidemiology2019; @YoungEvaluationMetaanalysisAir2019; @YoungReExtendedMortality2020].  For a critical assessment of this argument, see [redacted]<!--@HicksWhenVirtuesAre-->.  A second argument deploys a statistical method that Young and collaborators call "local control," which first clusters geographic units and then evaluates hazards separately within each cluster [@ObenchainAdvancingStatisticalThinking2013; @YoungBiasResponseHeterogeneity2015; @ObenchainLocalControlStrategy2017; @ObenchainLowlevelRadonExposure2019].  The local control method is not examined here.  

This paper focuses on the third argument made by Young and collaborators, which examines a collection of p-values, usually extracted from the primary studies in a meta-analysis or similar aggregate of studies [@YoungReliabilityEnvironmentalEpidemiology2019; @YoungAmbientAirPollution2019; @YoungCombinedBackgroundInformation2019; @YoungEvaluationMetaanalysisAir2019; @YoungReExtendedMortality2020; @YoungPM2AllcauseMortality2020] but sometimes produced by testing multiple distinct hypotheses on a single complex dataset [@YoungCerealinducedGenderSelection2009; @YoungAssessingGeographicHeterogeneity2013; @YouPM2OzoneIndicators2018]. The p-values are visualized using what Young and collaborators call a "p-value plot," and features of the plot are interpreted as indicating that there is no underlying effect [@YoungCerealinducedGenderSelection2009; @YouPM2OzoneIndicators2018; @YoungCombinedBackgroundInformation2019; @YoungReExtendedMortality2020; @YoungPM2AllcauseMortality2020], a heterogeneous mixture of zero and nonzero effects [@YoungReliabilityEnvironmentalEpidemiology2019; @YoungAmbientAirPollution2019; @YoungCombinedBackgroundInformation2019; @YoungEvaluationMetaanalysisAir2019], or that the authors of the primary studies have engaged in p-hacking [@YoungEvaluationMetaanalysisAir2019]. 

The first challenge in evaluating Young's p-value plot as a statistical method is the lack of exposition for the method.  Young and collaborators appear to rely almost entirely on visual inspection of their p-value plots, and give vague characterizations of how they think features of the plots should be interpreted.  Their interpretations are rarely given a clear justification.  At least one key term in their exposition is used in a non-standard way, and even the name "p-value plot" appears to be designed to associate the method used by Young and collaborators with two entirely different analyses of distributions of p-values.  

Therefore, in this section, I develop a more precisely-defined set of methods, which can be implemented computationally and automatically reproduced in the simulation.  These methods are intended to be more exact and reproducible — and hence intersubjectively stable — interpretations of the visual inspections conducted by Young and collaborators.  Insofar as my versions are indeed more exact and intersubjectively stable versions of the methods used by Young and collaborators, and insofar as my versions do not provide evidence to support the claims made by Young and collaborators, then I take it we have very good reason to conclude that the methods used by Young and collaborators do not provide evidence either.  That is, we would not expect less rigorous methods to provide better evidence than more rigorous methods.  

I proceed as follows.  I first briefly review two other graphical methods, cited by Young and collaborators in an attempt to justify their own method.  I then turn to Young's p-value plot, defining the plot itself and showing how it differs from the other two methods, and then discussing the various ways Young and collaborators have interpreted it.  The contrasts between Young's p-value plot and the other methods are important because, I suspect, Young and collaborators have avoided scrutiny in the peer-review literature by appearing to use previously-validated methods.  But in fact they are using completely different methods, which therefore require validation.  

After discussing the plots I present the basic setup of the simulation, then explain my interpretation of Young's methods as reproducible, quantified analyses of simulated data.  

All three graphical methods take as input a set of $N$ p-values $\P = \{p_1, p_2, \ldots, p_N\}$, (nominally) produced by applying a given statistical hypothesis test to $N$ replications of a given study design, each replication drawing samples of size $n$ from a given population.  This corresponds to the simplest case of meta-analysis.  Thus the p-values in $\P$ are nominally samples from a single underlying distribution $p_i \sim P$.  Note that, if the real effect is zero $\delta = 0$, then $P$ is the uniform distribution on $[0,1]$.  

Young and collaborators have also applied Young's p-value plot to collections of p-values produced by, for example, conducting hundreds of possible analyses on a single dataset [@YoungCerealinducedGenderSelection2009].  This method resembles "multiverse analysis," which systematically examines how methodological decisions shape the outputs of statistical analysis [@SteegenIncreasingTransparencyMultiverse2016].  However, the point of multiverse analysis is to identify patterns in the associations between decisions and outputs.  Young and collaborators have drawn much simpler conclusions, such as that "all of the p-values are simply the result of chance" [@YoungCerealinducedGenderSelection2009].  Because simulating these applications would require modeling complex multivariate distributions, I do not consider these applications here.  

A brief note on the name "Young's p-value plot."  With one exception, every peer-reviewed publication using the method has been written by a collection of authors.  In each of these works the method has been referred to as a "p-value plot," without attributing it to any individual.  However, Young is the only author who appears across all of these papers, and the method needs to be distinguished from other, nominally similar methods.  I therefore refer to the method as "Young's p-value plot."  

## Schweder and Spjøtvoll's p-value plot ##

Young and collaborators have frequently cited the "p-value plot" presented in @SchwederPlotsPvaluesEvaluate1982.  For this p-value plot, let $rank_{desc}(p_i)$ be the (1-indexed) *descending rank* of $p_i \in \P$, i.e., $rank_{desc}(p_i)$ is the number of p-values $p_j \in \P$ greater than or equal to $p_i$.  The largest p-value has descending rank 1, and the smallest p-value has descending rank $N$.  Then Schweder and Spjøtvoll's p-value plot plots the graph $(1-p_i, rank_{desc}(p_i))$.  Examples of Schweder and Spjøtvoll's p-value plot are included in the supplemental materials. 

@SchwederPlotsPvaluesEvaluate1982 briefly argue that the relationship between $1-p_i$ and $rank_{desc}(p_i)$ should be approximately linear "when [$p_i$] is not too small" and that, from left to right, "often, the plot will not show a clearcut break but rather a gradual bend" away from linearity.  Note that this means, as a method, Schweder and Spjøtvoll's p-value plot generally ignores small p-values, which roughly correspond to the statistically significant p-values.  They illustrate their method with example data and a straight line "drawn by visual fit" rather than regression analysis.    

## Simonsohn, Nelson, and Simmons' p-curve ##

Young and collaborators have regularly echoed concerns about the replication crisis unfolding in social psychology and certain areas of biomedical research [for example, @YoungReliabilityEnvironmentalEpidemiology2019 50].  In particular, they appeal to concerns about p-hacking, in which researchers conduct variant analyses (e.g., including or excluding various controls in a regression analysis) and report only the analyses with statistically significant p-values [@SimonsohnPcurveKeyFiledrawer2014].  Note that, other than Young's p-value plot, Young and collaborators have provided no specific empirical evidence of p-hacking in environmental epidemiology, and to my knowledge no such evidence has been published.[^phacking]  

[^phacking]: Young and collaborators do cite @HeadExtentConsequencesPHacking2015, which used text mining methods to examine p-values reported in "all Open Access papers available in the PubMed database," classified at the journal level into 22 disciplines.   @HeadExtentConsequencesPHacking2015 did not report the full size of their sample, but did include tens of thousands of p-values from "Medical and health sciences," which likely includes epidemiology but also other fields with very different methods, e.g., small-n animal model experiments and industry-funded clinical trials.  While their statistical tests did find evidence of p-hacking in "Medical and health sciences," they did not consider subfields.  Based on the methodological and funding diversity of the fields covered, and the lack of information about the distribution of subfields within their sample, we should be hesitant about drawing inferences to subfields.  

Young and collaborators have attempted to associate Young's p-value plot with a method developed to detect p-hacking, called a "p-curve" [@SimonsohnPcurveKeyFiledrawer2014; for a comparison of several methods to detect p-hacking, see @McShaneAdjustingPublicationBias2016].  The intuition behind the p-curve is that p-hacking will tend to produce an excess number of p-values "just below" the conventional $0.05$ threshold for statistical significance.  Formally, Simonsohn et al.'s p-curve first divides the interval $[0, 0.05]$ into 5 bins at the thresholds $0.01, 0.02, 0.03, 0.04, 0.05$, then calculates $N_b$, the number of p-values in bin $b$.  The p-curve is the graph $(b_t, N_b)$, where $b_t$ is the threshold for bin $b$.  The method then formally tests for p-hacking by applying statistical tests of the null hypothesis that the restricted distribution $P|_{p < 0.05}$ is uniform.  Examples of Simonsohn et al.'s p-curve using simulated data are included in the supplemental materials. 

Note that the p-curve is equivalent to a histogram on the interval $[0, 0.5]$ with binwidth $0.01$.  Also, the p-curve only includes statistically significant p-values.  Thus the p-curve and Schweder and Spjøtvoll's p-value plot not only produce different kinds of plots using the same input data \P, but actually direct their attention to different — typically disjoint — subsets of \P.  The two kinds of plots cannot be equivalent to each other.  At no point have Young and collaborators acknowledged this fundamental difference between the two methods that they cite as support for their own method.  

## Young's p-value plot ##

For Young's p-value plot, let $rank_{asc}(p_i)$ be the (1-indexed) *ascending rank* of $p_i \in \P$, i.e., $rank_{asc}(p_i)$ is the number of p-values $p_j \in \P$ less than or equal to $p_i$.  The smallest p-value has ascending rank 1, and the largest p-value has ascending rank $N$.  Without loss of generality, if \P is already in ascending order $p_1 < p_2 < \cdots < p_N$, then $rank_{asc}(p_i) = i$.  And Young's p-value plot is the graph $(i, p_i)$.  

Statistically-minded readers might have already noted that Young's p-value plot is a rescaled QQ-plot of $\P$ against the uniform distribution, with the theoretical quantiles $q_i = \frac{i}{N} = \frac{rank_{asc}(p_i)}{N}$.  It is not equivalent to the other two plots.  First, $rank_{desc}(p_i) = N - rank_{asc}(p_i) + 1$, and so given the number of studies $N$ Young's p-value does have a 1-1 mathematical relationship to Schweder and Spjøtvoll's p-value plot.  In geometric terms, Young's p-value plot swaps the axes of Schweder and Spjøtvoll's p-value plot and reverses the direction of the ranking.  However, because the axes are switched, a regression line fit to Schweder and Spjøtvoll's p-value plot will not necessarily correspond to a regression line fit to Young's p-value plot.  Thus there is not a 1-1 relationship between the slopes of the two plots.  (A scatterplot of regression line slopes for the two plots, across all runs of the simulation, is included in the automatically reproduced analysis.)  Second, because it is in a 1-1 relationship with Schweder and Spjøtvoll's plot, and this plot is fundamentally different from Simonsohn, Nelson, and Simmons' p-curve, Young's p-value plot is also fundamentally different from the p-curve. 

Young and collaborators have given the following explanation of their analysis of Young's p-value plot:  "Evaluation of a p-value plot follows a logical path. Is the p-value plot homogeneous? If the points roughly [sic] on a 45-degree, they are homogeneous and consistent with randomness; a lessor slope with all points roughly on a line indicates a consistent effect even if some of the individual p-values are not considered statistically significant. If the effects differ, one from another, beyond chance, then the effects are heterogeneous" [@YoungReliabilityEnvironmentalEpidemiology2019 48].  

A "45-degree line" typically refers to the graph of an identity function or the line $y = x$, which forms a 45-degree angle with respect to both the x- and y-axes.  This interpretation makes sense for a QQ-plot of p-values against the uniform distribution on $[0, 1]$, where both axes are on the scale $[0, 1]$; here a slope of 1 indicates that the underlying distribution $P$ is uniformly distributed, which in turn indicates that the real effect is zero.  Strictly speaking, this interpretation does not make sense for Young's p-value plot, where the x-axis (rank) is on the scale $[1, N]$ and the y-axis (p-value) is on the scale $[0, 1]$, giving a maximum possible slope of $1/N$.  I will assume that a "45-degree line" generally means that the slope of the equivalent QQ-plot is 1.  Note that Young and collaborators appear to only evaluate slopes visually; they do not report fitting regression models or using any other quantitative methods to measure slopes.  

Young and collaborators have frequently identified a "hockey stick" pattern [@YoungReliabilityEnvironmentalEpidemiology2019; @YoungAmbientAirPollution2019; @YoungCombinedBackgroundInformation2019; @YoungEvaluationMetaanalysisAir2019], which has "small p-values to the lower left ... and then points ascending in a roughly 45-degree line"; here "45-degree line" seems to mean that the right-hand side of the plot is linear, even if it does not have a slope of 1 [@YoungEvaluationMetaanalysisAir2019 5].  Young and collaborators have taken this non-linear "hockey stick" pattern to indicate some combination of heterogeneous effects, p-hacking, and researcher misconduct.  On two occasions Young and collaborators have formally tested non-linearity by comparing a linear and quadratic regression using an F-test [@YoungReliabilityEnvironmentalEpidemiology2019 50; @YoungCombinedBackgroundInformation2019 18].  More often they appear to have identified the "hockey stick" pattern by visual inspection alone.  

Young and collaborators do not appear to have validated the p-value plot method in any peer-reviewed studies.  @YoungCombinedBackgroundInformation2019 (8-9) briefly reported a simulation study.  In my notation, each run of this simulation had $N = 15$ p-values drawn from a uniform distribution, that is, a condition in which the zero hypothesis $\delta = 0$ is true.  They conducted $NN = 10$ simulation runs, but did not report any aggregate or distributional statistics; instead they reported the smallest (ascending rank = 1) p-value and the p-value plot for each simulation run.  They did not report any simulations of cases where the real effect was non-zero or heterogeneous.  They also do not report making the simulation code available anywhere for checking reproducibility or extending their analysis.  


## The current simulation study ##

The current study is an automatically-reproduced simulation study of Young's p-value plot, systematically testing it across five kinds of cases using computational, automatically reproduced analyses. 

Each run of the simulation is composed of $N$ studies.  To make the study parameters easy to understand, each study is based on a two-sample t-test.  Two samples, each of size $n$, are drawn from Gaussian distributions with means 0 and $\delta$, respectively, and common standard deviation $\sigma$.  In the basic, homogeneous case, $\delta$ is the same for all studies in the simulation run, and when $\sigma = 1$ $\delta$ corresponds to the true effect size of the effect being studied.  

A *condition* of the simulation fixes the values of these parameters.  Conditions can be systematically varied to compare, e.g., different effect sizes, and multiple runs $NN$ in each condition allow us to analyze the statistical properties of Young's p-value plot within and across conditions.  For the primary analysis of the current paper, 5 different effect sizes are used — corresponding to zero, small, moderate, moderate-large effects, and a "mixed" or heterogeneous condition.  Each condition is simulated with $NN = 500$ runs.  500 runs per condition was chosen because this should produce good estimates of central tendency and variation with just a few minutes of computation time (approximately 10 minutes on the service used to automatically reproduce the analysis).  Still, 500 runs is probably too few to produce highly precise estimates; so, for example, we should not finely discriminate between estimated p-values of $.051$ and $.049$.  The primary conditions examined in the current study are summarized in [@tbl:params].  

| parameter | meaning                             | value(s) |
|:----------|:------------------------------------|---------:|
| $\delta$  | real effect size                    | $0, .2, .4, .6, \{0, .6\}$ |
| $\sigma$  | s.d. of samples                     | 1        |
| $n$       | study sample size                   | 60       |
| $N$       | num. studies in each run            | 20       |
| $NN$      | num. runs for each condition        | 500      |

Table: **Base parameter values used in the current simulation study.**  The real effect size of $\{0, .6\}$ indicates the "mixed" or heterogeneous condition, in which half the population have no response and half the population have a strong response. {#tbl:params}

The simulation can also simulate a "mixed" or heterogeneous condition, in which subsets of the population have different responses to the intervention or exposure.  The current study uses a $\{0, .6\}$ mixture, meaning that one subpopulation has a zero effect or no response to the intervention and the other has a medium-strong response of .6.  These two quite different values were chosen to create a mixture that is less likely to look like a homogeneous case; a condition with a $\{.3, .4\}$ mixture would be difficult to distinguish from a homogeneous .35 condition.  

When given this kind of mixture, in the simulation, an individual study draws its sample from one of the subpopulations, selected uniformly at random.  So, in expectation, half of the studies will find a zero effect and half the medium-strong effect.  Of course, in any given set of studies, there will be variation in the ratios of the two subpopulations.  

For simplicity, the simulation currently does not support a continuous mixture, that is, where the real effect for a given study is drawn from some continuous distribution.  Such an extension of the simulation could be implemented in future work.  I expect that, if Young's p-value plot is unable to distinguish the discrete $\{0, .6\}$ mixture from homogeneous mixtures, then it would be even less able to distinguish a continuous mixture.  

After generating the data, it is straightforward to construct Young's p-value plot.  I then analyze the plot four ways.  First, graphs can be assessed visually, which appears to be the primary approach taken by Young and collaborators.  I draw a small sample of simulation runs, uniformly at random, and display their plots for visual inspection.  Of course, visual assessment cannot be scaled up to thousands of replications or reproduced computationally.  

Second, to assess "gaps" in the plot, I calculate the largest difference in consecutive p-values $\max(p_{i+1} - p_i)$.  I classify a p-value plot as "gappy" if this largest difference is greater than a threshold value, .125.  That is, if there is at least one visual gap of at least 12.5% in the p-value plot, the plot is considered "gappy."  While this threshold is arbitrary, I think it reasonably captures the sense of a visual "discontinuity" in the sequence of p-values. 

Third, slopes can be calculated using a simple univariate linear regression.  If the slope of the QQ-plot is approximately 1, this indicates that the underlying distribution of p-values $P$ is uniform, which in turn indicates that the zero hypothesis is true.  Because the slope of Young's p-value plot is $N$ times the slope of the QQ-plot, analysis of the QQ-plot is simpler than analyzing Young's p-value plot directly.  

After fitting a linear regression to the QQ-plot, I assess its slope in several ways, to judge whether it is "approximately 1."  First, I consider simply whether the slope is in the range $1\pm0.1$.  This interval is arbitrary, but I don't think it is an unreasonable operationalization of "approximately 1."  Next, I apply a t-test against the null hypothesis that the slope is exactly 1.  I also apply an equivalence test, the TOST procedure [@LakensEquivalenceTestingPsychological2018], against the null hypothesis that the slope is "substantially different" from 1.  The TOST (two one-sided test) procedure first requires defining an equivalence interval; here I use the interval $1\pm0.1$ again, as an arbitrary but not unreasonable range of values "approximately 1."  The procedure then conducts two one-sided t-tests, first against the null hypothesis that the value of interest is greater than the left-hand bound (that is, that the slope is greater than 1.1) and second against the null hypothesis that the value of interest is less than the right-hand bound (that is, the slope is less than .9).  The observed estimate passes the TOST test if and only if both one-sided t-tests are statistically significant.  If the estimate passes, we infer that the true value is within the equivalence interval, that is, "approximately 1." Finally, I use a Kolmogorov-Smirnov test (KS-test) to compare the observed set of p-values \P to the uniform distribution.  I use the conventional 0.05 threshold for statistical significance for all three tests.  When the estimate is not statistically significant, the tests as implemented in the simulation accept the null hypothesis; while strictly problematic, this approach simplifies the presentation and analysis of results and aligns with the way hypothesis tests are used in practice.  Because we are interested in these tests independently — as though the other analyses of the slope were not conducted — there is no need to correct for multiple comparisons.  

The fourth analytical approach is to evaluate the linearity of the plot.  Young and collaborators take non-linearity as evidence of some combination of heterogeneity, p-hacking, and researcher misconduct.  Linearity of a plot can be tested by fitting two regression models to its graphs — one linear, one quadratic — and selecting the model with the better fit.  I use two standard methods of model selection to compare regression models, AIC and an F-test.  I use the conventional 0.05 threshold for statistical significance in the F-test, and accept the null (linear model) when the F-test is not significant.  While AIC is a random variable, and could be compared more rigorously using a testing framework, I follow the way AIC is used in practice and simply compare the point estimates.  As with slope, for simplicity I analyze the QQ-plot rather than Young's p-value plot directly.  



# Measuring Evidence #

Again, Young and collaborators have interpreted Young's p-value plot as providing evidence for zero effects, heterogeneity, and p-hacking in the epidemiological literature on air pollution hazards.  Within the simulation, zero effects correspond to $\delta = 0$ and heterogeneity is represented with the $\delta = \{0, 0.6\}$ condition.  This allows us to quantitatively assess evidence for these conditions as hypotheses.  The simulation does not have a way to represent p-hacking, publication bias, or researcher misconduct; however, this means that all conditions represent cases in which these factors are false.  

I use two quantifiable conceptions of evidence that have been widely discussed in the philosophy of science literature, severity and likehood ratios.  These two conceptions of evidence are typically treated as philosophical rivals and associated with different philosophies of probability and statistics.  Using both allows me to remain agnostic to these disagreements.  

Attempts to quantify evidence — and especially the interpretation of quantified evidence using thresholds, as I do below — may remind some readers of the literature on inductive risk.  Consider the choice to set the threshold for a "severe test" at .05, .01, or .005 [compare @BenjaminRedefineStatisticalSignificance2018].  Setting this threshold at different values will lead to different patterns of acceptance and rejection of claims, which will have downstream non-epistemic consequences for, say, air pollution regulation.  There is widespread agreement among philosophers of science today that these non-epistemic consequences should be taken into account when the thresholds are set upstream [@DouglasSciencePolicyValuefree2009; @ElliottExploringInductiveRisk2017].  At a minimum, this suggests that I should not adopt conventional thresholds (as I do below) without considering whether they are appropriate given the downstream non-epistemic consequences at hand.  

However, in an analysis of climate politics, @HicksScientificControversiesProxy2017 observes that there is disagreement about the relative importance of different kinds of downstream non-epistemic consequences.  Air pollution politics is marked by similar disagreement.  Thus, applying inductive risk considerations here would require an extensive ethical argument for the relative importance of different kinds of consequences.  Given space constraints, such an argument is beyond the scope of this paper.  One advantage of conventions is that they can be somewhat acceptable from a wide variety of perspectives on a controversy.  I therefore adopt the conventions, recognizing that a more adequate analysis would address inductive risks directly.  

## Severity ##

The severity conception of evidence is associated with philosopher of science Deborah Mayo's reconceptualization of frequentist hypothesis testing [@MayoErrorGrowthExperimental1996; @MayoStatisticalInferenceSevere2018].[^blurb]  In its most recent form, the severity conception of evidence involves two "severity requirements":  

[^blurb]: Young provided a positive blurb for @MayoStatisticalInferenceSevere2018, stating that "Her severity requirement [sic] demands that the scientist provide a sharp question and related data. Absent that, the observer should withhold judgment or outright reject." 

- *Severity (weak)*: One does not have evidence for a claim if nothing has been done to rule out ways the claim may be false. If data $x$ agree with a claim $C$ but the method used is practically guaranteed to find such agreement, and had little or no capability of finding flaws with $C$ even if they exist, then we have bad evidence, no test (BENT). [@MayoStatisticalInferenceSevere2018 5]
- *Severity (strong)*: We have evidence for a claim $C$ just to the extent it survives a stringent scrutiny. If $C$ passes a test that was highly capable of finding flaws or discrepancies from $C$, and yet none or few are found, then the passing result, $x$, is evidence for $C$. [@MayoStatisticalInferenceSevere2018 14]

On this conception of evidence, a test or analytical method $T$ with observed output $t$ can provide evidence supporting a hypothesis $H$ only if $T$ would have given a different output if $H$ were false.  Hypothesis testing assesses this hypothetical condition in the form of a p-value using a mathematical model of the data-generating process: $p = pr(T = t | \lnot H) = L(\lnot H; T = t)$, where the role of $\lnot H$ is played by a null hypothesis $H_0$, which often states that some quantity of interest is zero.  A small p-value ($< 0.05$) indicates that the hypothetical is probably true, that is, if $H$ were false then $T$ would probably have given a different output.  On the other hand, a large p-value indicates that the test "is practically guaranteed" to produce this output, and so in this case by the weak severity principle "we have bad evidence, no test." 

(Note that, throughout this paper, I am distinguishing a *zero hypothesis* — that some effect is zero — from a *null hypothesis* — the alternate or rival hypothesis used to calculate a p-value.  Null hypotheses can claim that some quantity of interest is equal to a non-zero value.  For example, if we are using the slope of a QQ-plot to test whether a set of p-values is uniformly distributed, the null hypothesis is that the slope is 1.)

Severity can also be evaluated informally when a p-value cannot be calculated.  Consider visual features of plots, such as the "hockey stick" pattern that Young and collaborators interpret as evidence of heterogeneity.  It's not clear how to quantitatively determine whether this visual pattern is present in a given plot.  However, if this pattern is (qualitatively) common in homogeneous cases, then the weak severity principle implies that this visual pattern does not provide evidence of heterogeneity.  

The severity conception of evidence can be applied to Young and collaborator's skeptical claims about air pollution hazards as follows.  The claims $H$ are the zero hypothesis $\delta = 0$ and mixture hypothesis $\delta = \{0, .6\}$.  The method or test $T$ is Young's p-value plot, analyzed visually or in terms of slopes or linearity; the outputs $t$ that I will consider are (i) the "hockey stick" pattern for the visual analysis; (ii) "gaps" in the plot, based on a visual analysis and the size of the largest difference in consecutive p-values; (iii) slope of approximately 1 (on the QQ-plot), assessed based on whether the slope (as estimated using linear regression) falls into the range $1\pm0.1$, non-statistically significant results for a Z-test against the null hypothesis that the slope is exactly 1, and non-statistically significant results for the KS-test of uniformity; and (iv) non-linearity inferences using AIC and the F-test.  

[@Tbl:outputs] summarizes the outputs examined in the current study.  

|     | output            | determined using      | taken as evidence for |
|----:|:------------------|:----------------------|:----------------------|
|   i | "hockey stick"    | visual inspection     | mixed effect          |
|  ii | "gaps"            | visual inspection     | p-hacking or other problems |
|     |                   | largest gap $> .125$  ||
| iii | slope $\approx$ 1 | range $1\pm0.1$       | zero effect           |
|     |                   | T-test not stat. sig. ||
|     |                   | TOST test stat. sig.  || 
|     |                   | KS-test not stat. sig.|| 
|  iv | non-linearity     | AIC: quadratic        | mixed effect
|     |                   | F-test: stat. sig.    ||

Table: **Outputs of Young's p-value plot examined using the simulation.**  "Outputs" are features of plots that Young and collaborators take as evidence for critical assessments of air pollution epidemiological studies.  The "determined using" column indicates how these outputs are identified as present/absent in the current study.  {#tbl:outputs}

To generate p-values (or informally assess severity for the visual analysis), we need to specify a null hypothesis for $\lnot H$.  I will use each of the following:  

a. weak effect: $\delta = 0.2$
b. moderate effect: $\delta = 0.4$
c. strong-moderate effect: $\delta = 0.6$
d. non-zero effect: $a \vee b \vee c$
e. mixed effect: $\delta = \{0, .6\}$ for $H: \delta = 0$ and vice-versa (that is, the other skeptical hypothesis)
f. any other effect: $d \vee e$ (any of the non-zero effects or the other skeptical hypothesis)

In each case, insofar as the p-value is large $p > .05$, the simulation results indicate that this test output is common in the null hypothesis case, and so the weak severity criterion implies that Young's p-value plot does not provide evidence to support the skeptical claim made by Young and collaborators.  

## Likelihood ##

The likelihood conception of evidence is not strongly associated with any one statistician or philosopher of science, though it can be associated with one approach to Bayesian statistics [@KassBayesFactors1995; @RomeijnPhilosophyStatistics2017].  

Formally, the likelihood conception of evidence compares two rival hypotheses $H_1$ and $H_2$ using some data $d$.  The likelihood ratio is defined as
$$ K(H_1, H_2; d) = \frac{L(H_1; d)}{L(H_2; d)} = \frac{pr(d | H_1)}{pr(d | H_2)}. $$
If $K > 1$, then the evidence favors $H_1$; and $K < 1$ then the evidence favors $H_2$.  Sometimes $\log K$ is used to create symmetry between $H_1$ and $H_2$.  On one common interpretive scale, $\left|\log_{10} K\right| < 0.5$ is "not worth more than a bare mention," not supporting either hypothesis; $0.5 < \left|\log_{10} K\right| < 1$ is "substantial"; $1 < \left|\log_{10} K\right| < 2$ is "strong"; and $2 < \left|\log_{10} K\right|$ is "decisive" [@KassBayesFactors1995]. 

To apply the likelihood conception of evidence to Young and collaborators' skeptical claims about air pollution, $H_1$ will be the zero or mixture hypothesis, the rival hypothesis $H_2$ will be the hypotheses (a-f), and the data $d$ will be the analysis outputs (i-iv).  (For simplicity, the same dichotomous frequentist test outputs are used, e.g., statistically significant or not, rather than continuous-valued likelihoodist or Bayesian alternatives.)  In each case, insofar as $K < 0.5$, this implies that Young's p-value plot does not provide evidence to support the zero or mixture hypotheses.  

## Reproducibility ##

The simulation, analysis, and outputs (figures and tables) are publicly available and automatically reproduced.  Code is available at [redacted] and the automatically reproduced analysis can be viewed at [redacted].  
<!--<https://github.com/dhicks/p_curve> <https://dhicks.github.io/p_curve/>-->

The simulation and analysis are both written in R version 4.0.0 [@RCoreTeamLanguageEnvironmentStatistical2018] and make extensive use of the `tidyverse` suite of packages for R version 1.3.0 [@WickhamTidyverseEasilyInstall2019].  The TOSTER package version 0.3.4 [@LakensTOSTERTwoOneSided2018] was used to conduct the TOST analysis.  Because the software on the virtual machine used to automatically reproduce the analysis is updated each time the analysis is re-run, software versions reported online may be different from those reported here. 



# Simulation results #

Figures and tables included in the automatically reproduced analysis validate the simulation, showing that both the individual studies and meta-analyses (average within each simulation run) produce unbiased estimates of the real effect size $\delta$.  The only exception is the mixed case, where half of the population experiences zero effect ($\delta = 0$) and half the population experiences a medium-strong effect ($\delta = 0.6$).  In this case the average effect is 0.3, and in expectation the simulation gives this estimate.  


## Visual analyses ##

[@Fig:samples_young] shows 25 examples of Young's p-value plot across the five conditions, and [@fig:composite_young] shows Young's p-value plot across all runs of the simulation.  (See the supplemental material for examples of Schweder and Spjøtvoll's and Simonsohn et al.'s plots.)  

![**Examples of Young's p-value plot.**  Drawn at random from the simulation results.  Rows and colors correspond to conditions or real effects ($\delta$), from zero (0) to moderate-strong (0.6) and a mixed condition $\delta = \{0.0, 0.6\}$.  Columns correspond to indices for the simulation runs that produced these results, and are not meaningful.  (In particular, there is no relationship between simulation run $j$ in condition $a$ and simulation run $j$ in condition $b$.)  Each point corresponds to a single p-value in the meta-analysis (simulation run); the x-axis is the ascending rank of the p-value in the set \P, and the y-axis is the p-value itself.](fig_1_samples_young.png){ #fig:samples_young width=6in height=4in }

![**Composite of Young's p-value plot.**  Each individual line is Young's p-value plot for a single run of the simulation; all simulation runs are shown here.  Panels correspond to conditions or real effects ($\delta$), from zero (0) to moderate-strong (0.6) and a mixed condition $\delta = \{0.0, 0.6\}$.](fig_2_young_composite.png){ #fig:composite_young width=6in height=4in }

As preliminary observations, note that there are both substantial qualitative differences within effect sizes as well as substantial qualitative similarity across consecutive effect sizes.  Across all conditions, plots tend to have both statistically significant and insignificant p-values.  Larger effect sizes have more statistically significant results, resulting in a series of small p-values that gradually bend up.  The top two rows of [@fig:samples_young] and the first two panels of [@fig:composite_young] indicate that even zero and small effects can look nonlinear.  Comparing the composite plots ([@fig:composite_young]) for the mixed condition and the moderate effects condition, on average the mixed condition tends to produce a sharper bend upwards.  However, [@fig:samples_young] indicates that an individual moderate effects plot can have a sharp bend (index 327, far-right column) and a mixed effects plot can have a gradual bend (index 49, second column from the left).  

I focus on two visual patterns that Young and collaborators frequently discuss in their critiques of air pollution epidemiology:  (i) the "hockey stick" pattern; (ii) "gaps" in the plot.  

Young and collaborators take the "hockey stick" pattern to be evidence of heterogeneity.  The "hockey stick" comprises a more-or-less flat series of small p-values on the left (supposedly corresponding to the mixture component with a real effect) and a second series of steeply increasing p-values on the right (supposedly corresponding to the mixture component with zero effect).  This pattern is clearly visible in all of the example plots for the moderate, strong-moderate, and mixed effects (bottom three rows).  Because the "hockey stick" pattern appears in plots where there is only a single homogeneous effect, the weak severity criterion implies that *the hockey stick pattern does not provide evidence of a mixed or heterogeneous case*.  

Comparing panels in [@fig:composite_young], it seems reasonable to me to say that *on average* the mixed effect curve starts to visually bend up somewhat earlier — around study 7-12 — than the moderate and moderate-strong effects curves — around study 15 for moderate effects and study 18 for moderate-strong effects.  However, as [@fig:samples_young] indicates, any individual plot can bend up somewhat "earlier" or "later" than average.  Even if simple visual inspection could produce an intersubjectively precise judgement of when a given plot starts to bend, it would take further analysis to determine whether the "bend point" of a given plot is typical or unusual, relative to a hypothesized effect size.  Recall the first sentence of Mayo's definition of the weak severity criterion:  "One does not have evidence for a claim if nothing has been done to rule out ways the claim may be false" [@MayoStatisticalInferenceSevere2018 5].  Without an analysis of the location of "bend points" across different conditions, the antecedent in this version of weak severity is true, and so again the hockey stick pattern does not provide evidence of a mixed or hetergeneous case. 

Young and collaborators take visual "gaps" in the plot to be evidence of p-hacking, publication bias, or other questionable researcher practices [@YoungAssessingGeographicHeterogeneity2013; @YoungCombinedBackgroundInformation2019; @YoungEvaluationMetaanalysisAir2019].  These gaps are quite common in [@fig:samples_young], across all conditions.  (Because @fig:composite_young uses continuous lines rather than discrete points, the gaps are less striking.)  Note that the simulation does not include p-hacking, publication bias, or other questionable researcher practices.  Thus the weak severity criterion implies that *gaps in the plot do not provide evidence of p-hacking, publication bias, or other questionable researcher practices*.  



## Computationally reproducible analyses ##

Three outputs of the analysis of Young's p-value plot can be quantified and reproduced computationally:  (ii) "gaps" larger than .125 or 12.5 percentage points, (iii) a slope of approximately 1 (corresponding to the "45-degree line" as it is typically understood), and (iv) non-linearity.  Again, Young and collaborators take visual "gaps" in the plot to be evidence of p-hacking, publication bias, or other questionable researcher practices [@YoungAssessingGeographicHeterogeneity2013; @YoungCombinedBackgroundInformation2019; @YoungEvaluationMetaanalysisAir2019]; present compatibility with a "45-degree line" as evidence for zero effect [@YoungCerealinducedGenderSelection2009; @YouPM2OzoneIndicators2018; @YoungCombinedBackgroundInformation2019; @YoungReExtendedMortality2020]; and non-linearity as evidence for heterogeneity or mixed effects 
[@YoungReliabilityEnvironmentalEpidemiology2019; @YoungAmbientAirPollution2019; @YoungCombinedBackgroundInformation2019; @YoungEvaluationMetaanalysisAir2019].  For (iii) and (iv) I use the QQ-plot corresponding to Young's p-value plot, assessing a slope of 1 in four ways and non-linearity in two ways, as discussed above.  

### Severity analysis

@Fig:evidence_severity show the results of the severity analysis as p-values; see the supplemental materials for a table version of these results.  By the weak severity criterion, when the results of the severity analysis are greater than .05 (above the dashed line), the test output does not provide evidence in support of the target hypothesis.  

![**Results of the severity analysis for outputs (ii) gaps in the plot, (iii) slope of 1, and (iv) non-linearity.**  Severity analysis results are reported as p-values:  small values (conventionally $< .05$) indicate a severe test with respect to the null hypothesis.  Panels correspond to null hypotheses, and y-axis values correspond to the severity assessment (as a p-value) for the output with respect to the given null hypothesis.  The dashed line indicates $p = 0.05$; *points below this line indicate severe tests*.](fig_3_evidence_severity.png){#fig:evidence_severity width=6in height=4in}

Unspecified problems of p-hacking, publication bias, or other questionable researcher practices are supposedly supported by gaps in the plot.  The p-value for the presence of these gaps (output ii-gap) is greater than .25 for every null hypothesis, indicating that gaps are quite common.  In several conditions the majority of p-value plots are "gappy."  Thus *gaps in the plot do not provide evidence of p-hacking, publication bias, or other questionable researcher practices.*  

The zero effect hypothesis is supposedly supported by a slope of approximately 1 on the QQ-plot.  The range method (whether the slope is within the range $1\pm0.1$, output iii-range) and the KS-test (output iii-KS) are severe against the null hypotheses of moderate and moderate-strong effects ($\delta = 0.4$ and $\delta = 0.6$); the t-test method (output iii-T) is only severe against moderate-strong effects ($\delta = 0.6$).  The TOST test does a better job, providing a severe test against all combinations of the non-zero hypotheses.  *So the "45-degree line" may or may not provide evidence for zero effects, depending on the particular null hypothesis being tested and particular test used.*  

This means that, in order to determine whether the "45-degree line" provides evidence for zero effects, the analyst must be explicit about both the test method used and the null hypothesis being tested.  A slope of .92, say, will not provide evidence if the null hypothesis is a small effect.  In addition, while these results recommend using the TOST test — which can provide evidence even when the null hypothesis is a small effect — this test requires an explicit equivalence interval, the range of values that are "approximately 1."  But Young and collaborators are not explicit in any of these requisite ways. Indeed, they never report calculating a slope, using a regression line, or other method, much less conducting some further analysis of that slope.  Instead, they present the p-value plot, state some version of the principle that "If the points fall on a 45-degree line, then the results are consistent with randomness" [@YoungCombinedBackgroundInformation2019 194], and move directly to some version of the conclusion that "The p-value plot for these data is consistent with pure randomness" [@YouPM2OzoneIndicators2018 193; compare @YoungCerealinducedGenderSelection2009; @YoungCombinedBackgroundInformation2019; @YoungReExtendedMortality2020]. At best, this means that the evidentiary value of their analysis is ambiguous.  We would need to know more in order to judge whether they have produced evidence to support a zero effect.  However, I take it that the lack of detail is a good reason to think that they have simply assessed the slope visually.  A visual assessment will be even less sensitive than calculating a slope and determining whether it is in the range $1\pm.1$, which can only provide evidence against a relatively large null hypothesis.  *So in cases where a small effect is a live possibility, insofar as Young and collaborators are relying on visual judgment, the "45-degree line" does not provide evidence of a zero effect.* 

The mixed-effect hypothesis, or heterogeneity, is supposedly supported by non-linearity.  Two quantified versions of this output are examined here, comparing linear and quadratic regressions using AIC (iv-AIC) and an F-test (iv-F).  The AIC and F-test evaluations of non-linearity do not provide severe tests against any of the alternative conditions.  That is, *neither of the tests of linearity considered here provide evidence for heterogeneity*.  



### Likelihood analysis

@Fig:evidence_likelihood show the results of the likelihood analysis; see the supplemental materials for a table version of these results.  Log likelihood ratios are reported, so results above $0.5$ support $H_1$ and results below $-0.5$ support $H_2$.  So, by the weak severity criterion, when the results of the likelihood analysis are $< 0.5$, the test output does not provide evidence supporting the target hypothesis of zero or mixed effect.  

![**Results of the likelihood analysis for outputs (iii) slope of approximately 1 and (iv) non-linearity.**  Each point gives the log likelihood ratio for a target vs. rival hypothesis, given an output.  Each panel represents one combination of a target hypothesis $H_1$ — either a zero effect $\delta = 0$ or a mixed effect — and a rival hypothesis $H_2$.  Target hypotheses are organized in rows; rival hypotheses are organized in columns.  Point color corresponds to rival hypothesis $H_2$.  Position on the x-axis indicates the output. Position on the y-axis indicates the strength of the evidence that the output provides to the hypotheses:  *greater values indicate more support for $H_1$ over $H_2$*.  (Points at the plot margins have infinite value due to division by zero.)  Shaded regions indicate the degree of support for one hypothesis against the other, in order from lightest to darkest:  none, "substantial," "strong," "decisive."  An interactive version of this plot is included in the automatic reproduction of the analysis for this paper.](fig_4_evidence_likelihood.png){#fig:evidence_likelihood  width=6in height=4in }

For "gaps" in the plot, calculating the likelihood ratio would require simulation conditions that included p-hacking and other questionable research practices.  Because the simulation does not currently support these kinds of conditions, likelihood analysis cannot be used for this output.  
 
The zero effect hypothesis (upper row) is supposedly supported by a slope of approximately 1.  All four methods provide "decisive" support for a zero effect against moderate-strong effects, and "substantial" or better support against moderate effects.  The TOST approach provides stronger or equally strong evidence, compared to the other approaches, across all of the rival hypotheses.  In general, when the rival hypothesis includes weak effects, other approaches either do not provide evidence to support zero effects, or just barely provide "substantial" evidence.  So, as with the severity analysis, *whether and to what degree the "45-degree line" may or may not provide evidence for zero effects depends on the choice of rival hypothesis and analytical approach used*. In addition, and again as in the severity analysis, the visual assessment apparently used by Young and collaborators will provide weaker evidence than the range test, which does not provide evidence against rivals that include small effects.  *So, against rival hypotheses that include small effects, insofar as Young and collaborators are relying on visual judgment, the "45-degree line" does not provide evidence of a zero effect.* 

For the mixed effect hypothesis, all of the points for both AIC and the F-test are in the "not worth mentioning" or no evidence range, and so *neither method provides evidence to support heterogeneity*.  This is the same conclusion reached by the severity analysis. 



# Discussion #

This simulation analysis finds that Young's p-value plot does not provide evidence for heterogeneity or p-hacking based on the "hockey stick" shape, "gaps" in the plot, or AIC or F-tests of non-linearity.  The method can provide evidence for zero effects based on a slope of 1, depending on what rival or null hypothesis is considered and how the plot is analyzed.  In general, producing evidence for zero effects against small effects requires using the TOST approach; visual inspection alone will not be sufficient.  This approach requires setting an explicit range of values within which the slope is considered "approximately 1."  

In the meta-analyses criticized by Young and collaborators, the estimated short-term effects of air pollution are small on a relative risk scale.  @NawrotPublicHealthImportance2011 estimated the effect for air pollution (increase of 10 $\mu g/m^3$ PM$_{10}$) on non-fatal myocardial infarction to be 1.02 (95% CI 1.01–1.02); the point estimates for six pollutants reported by @MustaficMainAirPollutants2012 (increase of 10 $\mu g/m^3$ for all except carbon monoxide) were all in the range 1.003-1.048; @LiuAmbientParticulateAir2019 estimated effects for PM$_{10}$ (increase of 10 $\mu g/m^3$) on all-cause mortality of 1.044 (95% CI 1.039-1.050); and @OrellanoShorttermExposureParticulate2020 estimated effects for PM$_{2.5}$ on all-cause morality of 1.0065 (95% CI 1.0044–1.0086).  A precise conversion from risk ratios to Cohen's $d$ (equivalent to $\delta$ used in the simulations) is beyond the scope of this paper; however, using the rule of thumb that the risk ratio is approximately equal to the log odds when the outcome is rare [@VieraOddsRatiosRisk2008] and the conversion factor $\sqrt{3}/\pi$ between the log odds and Cohen's $d$, a risk ratio of 1.05 is roughly equivalent to $d=0.03$, which in turn is roughly an order of magnitude smaller than the smallest effect condition examined above.[^power]  This indicates that *visual inspection of Young's p-value plot alone is incapable of producing evidence against very small epidemiological effects of air pollution*.  

[^power]: In the reproducible analysis document, a supplemental analysis examines conditions with real effect sizes of $\delta = 0.05$ but with sample sizes in the primary studies that were severely underpowered (20% chance of correctly rejecting the null hypothesis of no effect), moderately underpowered (50% chance), or adequately powered (80% chance).  In severity terms, no analytical approach was capable of providing evidence when the primary studies were severely underpowered; the TOST approach was capable of providing evidence when the primary studies were moderately underpowered; and all approaches were capable of providing evidence when the primary studies were adequately powered.  Young and collaborators do not report a power analysis of any of the meta-analyses or primary studies that they criticize. 

However, Young and collaborators generally have not claimed zero effects in their criticisms of these air pollution epidemiological studies.  More often they have claimed to find heterogeneity, p-hacking, and publication bias [@YoungReliabilityEnvironmentalEpidemiology2019; @YoungAmbientAirPollution2019; @YoungEvaluationMetaanalysisAir2019].  The simulation results indicate that Young's p-value plot is incapable of providing evidence for any of these claims, using either visual inspection or either of the quantitative approaches examined here.  The features that Young and collaborators point to — the "hockey stick" shape, "gaps," non-linearity — are readily produced by moderate and stronger effects, and can even appear in zero and weak-effect conditions.  

All together, Young and collaborators have repeatedly drawn conclusions that their p-value plot method cannot support.  These errors could have been avoided had they first attempted to validate the method.  

Young and collaborators have been able to publish multiple papers in peer-reviewed journals using an unvalidated method that is only capable of providing evidence to support the kinds of claims that they make under specific circumstances.  Ironically, Young and collaborators have repeatedly raised concerns about the inability of peer review to catch flawed scientific research [@YoungTestimonyCommitteeScience2012; @YoungStanleyYoungScientific2013; @MillerViewpointWhyMany2017; @YoungEvaluationMetaanalysisAir2019], and the evident purpose of Young's recent work on air pollution epidemiology has been to demonstrate these failures of peer review.  Young's p-value plot seems to have fallen through the cracks that he and his collaborators purport to find in the air pollution epidemiology literature.  

